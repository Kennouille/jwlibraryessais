from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from datetime import datetime
import os
import zipfile
import sqlite3
import shutil
import uuid
import time
import sys
import gc
import io
import traceback
import threading


app = Flask(__name__)
app.config['PROPAGATE_EXCEPTIONS'] = True
CORS(app, origins=["https://jwmerge.netlify.app"])

UPLOAD_FOLDER = "uploads"
EXTRACT_FOLDER = "extracted"

os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(EXTRACT_FOLDER, exist_ok=True)


def normalize_mapping_keys(mapping):
    return {
        (os.path.normpath(k[0]), k[1]): v
        for k, v in mapping.items()
    }


def get_current_local_iso8601():
    now_local = datetime.datetime.now()
    return now_local.strftime("%Y-%m-%dT%H:%M:%S")


def checkpoint_db(db_path):
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA wal_checkpoint(FULL)")
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"Erreur lors du checkpoint de {db_path}: {e}")


def list_tables(db_path):
    """
    Retourne une liste des noms de tables pr√©sentes dans la base de donn√©es
    sp√©cifi√©e par 'db_path', en excluant les tables syst√®me (commen√ßant par 'sqlite_').
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
        SELECT name
        FROM sqlite_master
        WHERE type='table'
          AND name NOT LIKE 'sqlite_%'
    """)
    tables = [row[0] for row in cursor.fetchall()]
    conn.close()
    return tables


def merge_independent_media(merged_db_path, file1_db, file2_db):
    """
    Fusionne la table IndependentMedia des deux bases sources dans la base fusionn√©e.
    Deux lignes sont consid√©r√©es identiques si (OriginalFilename, FilePath, Hash) sont identiques.
    Si une ligne existe d√©j√†, on ignore la nouvelle pour pr√©server les donn√©es existantes.
    Retourne un mapping : {(db_source, ancien_ID) : nouveau_ID, ...}
    """
    print("\n[FUSION INDEPENDENTMEDIA]")
    mapping = {}
    with sqlite3.connect(merged_db_path) as merged_conn:
        merged_cursor = merged_conn.cursor()

        for db_path in [file1_db, file2_db]:
            print(f"Traitement de {db_path}")
            with sqlite3.connect(db_path) as src_conn:
                src_cursor = src_conn.cursor()
                src_cursor.execute("""
                    SELECT IndependentMediaId, OriginalFilename, FilePath, MimeType, Hash
                    FROM IndependentMedia
                """)
                rows = src_cursor.fetchall()

                for row in rows:
                    old_id, orig_fn, file_path, mime, hash_val = row
                    print(f"  - M√©dia : {orig_fn}, Hash={hash_val}")

                    # V√©rifie si la ligne existe d√©j√† (√©vite doublons)
                    merged_cursor.execute("""
                        SELECT IndependentMediaId, MimeType
                        FROM IndependentMedia
                        WHERE OriginalFilename = ? AND FilePath = ? AND Hash = ?
                    """, (orig_fn, file_path, hash_val))
                    result = merged_cursor.fetchone()

                    if result:
                        new_id, existing_mime = result
                        # Au lieu de mettre √† jour le MimeType, on ignore simplement la nouvelle ligne
                        print(f"    > Ligne d√©j√† pr√©sente pour ID {new_id} (ignor√©e pour {db_path})")
                    else:
                        merged_cursor.execute("""
                            INSERT INTO IndependentMedia (OriginalFilename, FilePath, MimeType, Hash)
                            VALUES (?, ?, ?, ?)
                        """, (orig_fn, file_path, mime, hash_val))
                        new_id = merged_cursor.lastrowid
                        print(f"    > Insertion nouvelle ligne ID {new_id}")

                    mapping[(db_path, old_id)] = new_id

        merged_conn.commit()

    print("Fusion IndependentMedia termin√©e.")
    return mapping


def read_notes_and_highlights(db_path):
    if not os.path.exists(db_path):
        return {"error": f"Base de donn√©es introuvable : {db_path}"}
    checkpoint_db(db_path)
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT n.NoteId, n.Guid, n.Title, n.Content, n.LocationId, um.UserMarkGuid,
               n.LastModified, n.Created, n.BlockType, n.BlockIdentifier
        FROM Note n
        LEFT JOIN UserMark um ON n.UserMarkId = um.UserMarkId
    """)
    notes = cursor.fetchall()

    cursor.execute("""
        SELECT UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version
        FROM UserMark
    """)
    highlights = cursor.fetchall()

    conn.close()
    return {"notes": notes, "highlights": highlights}


def extract_file(file_path, extract_folder):
    zip_path = file_path.replace(".jwlibrary", ".zip")
    if os.path.exists(zip_path):
        os.remove(zip_path)
    os.rename(file_path, zip_path)
    extract_full_path = os.path.join(EXTRACT_FOLDER, extract_folder)
    os.makedirs(extract_full_path, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_full_path)
    return extract_full_path


def create_merged_schema(merged_db_path, base_db_path):
    checkpoint_db(base_db_path)
    src_conn = sqlite3.connect(base_db_path)
    src_cursor = src_conn.cursor()
    src_cursor.execute(
        "SELECT type, name, sql FROM sqlite_master "
        "WHERE type IN ('table', 'index', 'trigger', 'view') "
        "AND name NOT LIKE 'sqlite_%'"
    )
    schema_items = src_cursor.fetchall()
    src_conn.close()

    merged_conn = sqlite3.connect(merged_db_path)
    merged_cursor = merged_conn.cursor()
    for obj_type, name, sql in schema_items:
        # On exclut la table (et triggers associ√©s) LastModified
        if (obj_type == 'table' and name == "LastModified") or (obj_type == 'trigger' and "LastModified" in sql):
            continue
        if sql:
            try:
                merged_cursor.execute(sql)
            except Exception as e:
                print(f"Erreur lors de la cr√©ation de {obj_type} '{name}': {e}")
    merged_conn.commit()

    try:
        merged_cursor.execute("DROP TABLE IF EXISTS LastModified")
        merged_cursor.execute("CREATE TABLE LastModified (LastModified TEXT NOT NULL)")
    except Exception as e:
        print(f"Erreur lors de la cr√©ation de la table LastModified: {e}")
    merged_conn.commit()

    # Cr√©ation correcte de PlaylistItemMediaMap si elle n'existe pas
    merged_cursor.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='PlaylistItemMediaMap'"
    )
    if not merged_cursor.fetchone():
        merged_cursor.execute("""
            CREATE TABLE PlaylistItemMediaMap (
                PlaylistItemId   INTEGER NOT NULL,
                MediaFileId      INTEGER NOT NULL,
                OrderIndex       INTEGER NOT NULL,
                PRIMARY KEY (PlaylistItemId, MediaFileId),
                FOREIGN KEY (PlaylistItemId) REFERENCES PlaylistItem(PlaylistItemId),
                FOREIGN KEY (MediaFileId)  REFERENCES IndependentMedia(IndependentMediaId)
            )
        """)
        print("PlaylistItemMediaMap (avec MediaFileId, OrderIndex) cr√©√©e dans la base fusionn√©e.")

    merged_conn.commit()
    merged_conn.close()


def create_table_if_missing(merged_conn, source_db_paths, table):
    cursor = merged_conn.cursor()
    cursor.execute(f"PRAGMA table_info({table})")
    if cursor.fetchone() is None:
        create_sql = None
        for db_path in source_db_paths:
            checkpoint_db(db_path)
            src_conn = sqlite3.connect(db_path)
            src_cursor = src_conn.cursor()
            src_cursor.execute("SELECT sql FROM sqlite_master WHERE type='table' AND name=?", (table,))
            row = src_cursor.fetchone()
            src_conn.close()
            if row and row[0]:
                create_sql = row[0]
                break
        if create_sql:
            try:
                merged_conn.execute(create_sql)
                print(f"Table {table} cr√©√©e dans la base fusionn√©e.")
            except Exception as e:
                print(f"Erreur lors de la cr√©ation de {table}: {e}")
        else:
            print(f"Aucun sch√©ma trouv√© pour la table {table} dans les bases sources.")


def merge_other_tables(merged_db_path, db1_path, db2_path, exclude_tables=None):
    """
    Fusionne toutes les tables restantes (hors celles sp√©cifi√©es dans exclude_tables)
    dans la base fusionn√©e de mani√®re idempotente.
    Pour chaque table, la fonction v√©rifie si une ligne identique (comparaison sur toutes
    les colonnes sauf la cl√© primaire) est d√©j√† pr√©sente avant insertion.
    """
    if exclude_tables is None:
        exclude_tables = ["Note", "UserMark", "Bookmark", "InputField"]

    # On effectue un checkpoint pour s'assurer que les donn√©es sont bien synchronis√©es.
    checkpoint_db(db1_path)
    checkpoint_db(db2_path)

    def get_tables(path):
        with sqlite3.connect(path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
            return {row[0] for row in cursor.fetchall()}

    tables1 = get_tables(db1_path)
    tables2 = get_tables(db2_path)
    all_tables = (tables1 | tables2) - set(exclude_tables)

    merged_conn = sqlite3.connect(merged_db_path)
    merged_cursor = merged_conn.cursor()
    source_db_paths = [db1_path, db2_path]

    for table in all_tables:
        # Cr√©e la table dans la DB fusionn√©e si elle est manquante
        create_table_if_missing(merged_conn, source_db_paths, table)
        merged_cursor.execute(f"PRAGMA table_info({table})")
        columns_info = merged_cursor.fetchall()
        if not columns_info:
            print(f"‚ùå Table {table} introuvable dans la DB fusionn√©e.")
            continue

        # üîµ S√©curisation forte : tout est string forc√©e
        columns = [str(col[1]) for col in columns_info]
        columns_joined = ", ".join(columns)
        placeholders = ", ".join(["?"] * len(columns))

        for source_path in source_db_paths:
            with sqlite3.connect(source_path) as src_conn:
                src_cursor = src_conn.cursor()
                try:
                    src_cursor.execute(f"SELECT * FROM {table}")
                    rows = src_cursor.fetchall()
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur lecture de {table} depuis {source_path}: {e}")
                    rows = []

                for row in rows:
                    if len(columns) > 1:
                        where_clause = " AND ".join([f"{str(col)}=?" for col in columns[1:]])
                        check_query = f"SELECT 1 FROM {table} WHERE {where_clause} LIMIT 1"
                        merged_cursor.execute(check_query, row[1:])
                        exists = merged_cursor.fetchone()
                    else:
                        # Cas sp√©cial : table avec seulement cl√© primaire
                        exists = None

                    if not exists:
                        cur_max = merged_cursor.execute(f"SELECT MAX({str(columns[0])}) FROM {table}").fetchone()[
                                      0] or 0
                        new_id = int(cur_max) + 1
                        new_row = (new_id,) + row[1:]
                        print(f"‚úÖ INSERT dans {table} depuis {source_path}: {new_row}")
                        merged_cursor.execute(
                            f"INSERT INTO {table} ({columns_joined}) VALUES ({placeholders})", new_row
                        )
                    else:
                        print(f"‚è© Doublon ignor√© dans {table} depuis {source_path}: {row[1:]}")

    merged_conn.commit()
    merged_conn.close()


def merge_bookmarks(merged_db_path, file1_db, file2_db, location_id_map):
    print("\n[FUSION BOOKMARKS - ID√âMPOTENT]")
    mapping = {}
    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    # Table de mapping
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MergeMapping_Bookmark (
            SourceDb TEXT,
            OldID INTEGER,
            NewID INTEGER,
            PRIMARY KEY (SourceDb, OldID)
        )
    """)

    for db_path in [file1_db, file2_db]:
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()

            src_cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='Bookmark'")
            if not src_cursor.fetchone():
                print(f"Aucune table Bookmark trouv√©e dans {db_path}")
                continue

            src_cursor.execute("""
                SELECT BookmarkId, LocationId, PublicationLocationId, Slot, Title, 
                       Snippet, BlockType, BlockIdentifier
                FROM Bookmark
            """)
            for row in src_cursor.fetchall():
                old_id, loc_id, pub_loc_id, slot, title, snippet, block_type, block_id = row

                # D√©j√† fusionn√© ?
                cursor.execute("""
                    SELECT NewID FROM MergeMapping_Bookmark
                    WHERE SourceDb = ? AND OldID = ?
                """, (db_path, old_id))
                res = cursor.fetchone()
                if res:
                    mapping[(db_path, old_id)] = res[0]
                    continue

                # Nouveau LocationId mapp√©
                new_loc_id = location_id_map.get((db_path, loc_id), loc_id)
                new_pub_loc_id = location_id_map.get((db_path, pub_loc_id), pub_loc_id)

                cursor.execute("SELECT 1 FROM Location WHERE LocationId IN (?, ?)", (new_loc_id, new_pub_loc_id))
                if len(cursor.fetchall()) != 2:
                    print(f"‚ö†Ô∏è LocationId introuvable pour Bookmark OldID {old_id} dans {db_path} (LocationId {new_loc_id} ou PublicationLocationId {new_pub_loc_id}), ignor√©.")
                    continue

                # üîç V√©rification de doublon sur tous les champs SAUF PublicationLocationId et Slot
                cursor.execute("""
                    SELECT BookmarkId FROM Bookmark
                    WHERE LocationId = ?
                    AND PublicationLocationId = ?
                    AND Slot = ?
                    AND Title = ?
                    AND IFNULL(Snippet, '') = IFNULL(?, '')
                    AND BlockType = ?
                    AND IFNULL(BlockIdentifier, -1) = IFNULL(?, -1)
                """, (new_loc_id, new_pub_loc_id, slot, title, snippet, block_type, block_id))

                existing = cursor.fetchone()

                if existing:
                    existing_id = existing[0]
                    print(f"‚è© Bookmark identique trouv√© (m√™me contenu mais diff√©rent emplacement) : OldID {old_id} ‚Üí NewID {existing_id}")
                    mapping[(db_path, old_id)] = existing_id
                    cursor.execute("""
                        INSERT OR IGNORE INTO MergeMapping_Bookmark (SourceDb, OldID, NewID)
                        VALUES (?, ?, ?)
                    """, (db_path, old_id, existing_id))
                    continue

                # ‚ö†Ô∏è Sinon, v√©rifier et ajuster le slot
                original_slot = slot
                while True:
                    cursor.execute("""
                        SELECT 1 FROM Bookmark
                        WHERE PublicationLocationId = ? AND Slot = ?
                    """, (new_pub_loc_id, slot))
                    if not cursor.fetchone():
                        break
                    slot += 1

                print(f"Insertion Bookmark: OldID {old_id} (slot initial {original_slot} -> {slot}), PubLocId {new_pub_loc_id}, Title='{title}'")
                cursor.execute("""
                    INSERT INTO Bookmark
                    (LocationId, PublicationLocationId, Slot, Title,
                     Snippet, BlockType, BlockIdentifier)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (new_loc_id, new_pub_loc_id, slot, title, snippet, block_type, block_id))
                new_id = cursor.lastrowid
                mapping[(db_path, old_id)] = new_id

                cursor.execute("""
                    INSERT INTO MergeMapping_Bookmark (SourceDb, OldID, NewID)
                    VALUES (?, ?, ?)
                """, (db_path, old_id, new_id))

    # üëâ Un seul commit apr√®s TOUT
    conn.commit()
    conn.close()

    print("‚úî Fusion Bookmarks termin√©e (avec d√©tection de doublons par contenu).")
    return mapping


def merge_notes(merged_db_path, file1_db, file2_db, location_id_map, usermark_guid_map):
    """
    Fusionne la table Note de fa√ßon √† ne pas √©craser les donn√©es existantes.
    Si une note avec le m√™me GUID existe d√©j√† mais que le contenu diff√®re,
    on ins√®re une nouvelle note avec un nouveau GUID et on laisse en place la note existante.
    Renvoie un mapping (SourceDb, OldNoteId) -> NewNoteId
    """
    print("\n=== FUSION DES NOTES (r√©solution de conflit par insertion) ===")
    inserted = 0
    note_mapping = {}

    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    for db_path in [file1_db, file2_db]:
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("""
                SELECT n.NoteId, n.Guid, um.UserMarkGuid, n.LocationId, n.Title, n.Content,
                       n.LastModified, n.Created, n.BlockType, n.BlockIdentifier
                FROM Note n
                LEFT JOIN UserMark um ON n.UserMarkId = um.UserMarkId
            """)
            for (old_note_id, guid, usermark_guid, location_id, title, content,
                 last_modified, created, block_type, block_identifier) in src_cursor.fetchall():

                normalized_key = (os.path.normpath(db_path), location_id)
                normalized_map = {(os.path.normpath(k[0]), k[1]): v for k, v in location_id_map.items()}
                new_location_id = normalized_map.get(normalized_key) if location_id else None
                new_usermark_id = usermark_guid_map.get(usermark_guid) if usermark_guid else None

                if new_location_id is None:
                    print(f"‚ö†Ô∏è LocationId introuvable pour Note guid={guid} (source: {db_path}), ignor√©e.")
                    continue

                # V√©rifier si le GUID existe d√©j√†
                cursor.execute("SELECT NoteId, Title, Content FROM Note WHERE Guid = ?", (guid,))
                existing = cursor.fetchone()

                if existing:
                    existing_note_id, existing_title, existing_content = existing
                    if existing_title == title and existing_content == content:
                        print(f"Note guid={guid} d√©j√† pr√©sente et identique (source: {db_path}), aucune action.")
                        note_mapping[(db_path, old_note_id)] = existing_note_id
                        continue
                    else:
                        new_guid = str(uuid.uuid4())
                        print(f"Conflit pour Note guid={guid} (source: {db_path}). "
                              f"Insertion d'une nouvelle note avec nouveau GUID {new_guid}.")
                        guid_to_insert = new_guid
                else:
                    guid_to_insert = guid

                cursor.execute("""
                    INSERT INTO Note
                    (Guid, UserMarkId, LocationId, Title, Content,
                     LastModified, Created, BlockType, BlockIdentifier)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    guid_to_insert,
                    new_usermark_id,
                    new_location_id,
                    title,
                    content,
                    last_modified,
                    created,
                    block_type,
                    block_identifier
                ))
                new_note_id = cursor.lastrowid
                note_mapping[(db_path, old_note_id)] = new_note_id
                inserted += 1

    conn.commit()
    conn.close()
    print(f"‚úÖ Total notes ins√©r√©es: {inserted}")
    return note_mapping


def merge_usermark_with_id_relabeling(merged_db_path, source_db_path, location_id_map):
    conn_merged = sqlite3.connect(merged_db_path)
    cur_merged = conn_merged.cursor()

    # R√©cup√®re les IDs existants pour √©viter les conflits
    cur_merged.execute("SELECT UserMarkId FROM UserMark")
    existing_ids = set(row[0] for row in cur_merged.fetchall())
    current_max_id = max(existing_ids) if existing_ids else 0

    # Charge les donn√©es source
    conn_source = sqlite3.connect(source_db_path)
    cur_source = conn_source.cursor()
    cur_source.execute("SELECT UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version FROM UserMark")
    source_rows = cur_source.fetchall()
    conn_source.close()

    # Cr√©ation du mapping UserMarkId (si conflits)
    replacements = {}
    for row in source_rows:
        old_id = row[0]
        if old_id in existing_ids:
            current_max_id += 1
            new_id = current_max_id
            replacements[old_id] = new_id
        else:
            replacements[old_id] = old_id
            existing_ids.add(old_id)

    # Insertion dans la base fusionn√©e avec LocationId mapp√©
    for row in source_rows:
        old_id = row[0]
        new_id = replacements[old_id]
        ColorIndex = row[1]
        LocationId = row[2]
        StyleIndex = row[3]
        UserMarkGuid = row[4]
        Version = row[5]

        # Mapping du LocationId
        mapped_loc_id = location_id_map.get((source_db_path, LocationId), LocationId)

        try:
            cur_merged.execute("""
                INSERT INTO UserMark (UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (new_id, ColorIndex, mapped_loc_id, StyleIndex, UserMarkGuid, Version))
        except Exception as e:
            print(f"Erreur insertion UserMark old_id={old_id}, new_id={new_id}: {e}")

    conn_merged.commit()
    conn_merged.close()
    return replacements


def merge_blockrange_from_two_sources(merged_db_path, file1_db, file2_db):
    print("\n=== FUSION BLOCKRANGE ===")

    try:
        with sqlite3.connect(merged_db_path) as dest_conn:
            dest_conn.execute("PRAGMA busy_timeout = 10000")
            dest_cursor = dest_conn.cursor()

            # 1) V√©rification initiale
            dest_cursor.execute("SELECT COUNT(*) FROM BlockRange")
            print(f"BlockRanges initiaux: {dest_cursor.fetchone()[0]}")

            # 2) R√©cup√©ration des mappings
            dest_cursor.execute("SELECT UserMarkId, UserMarkGuid FROM UserMark")
            usermark_guid_map = {guid: uid for uid, guid in dest_cursor.fetchall()}
            print(f"UserMark GUIDs: {usermark_guid_map}")

            # 3) Traitement des sources
            for db_path in [file1_db, file2_db]:
                print(f"\nTraitement de {db_path}")
                try:
                    with sqlite3.connect(db_path) as src_conn:
                        src_cursor = src_conn.cursor()
                        src_cursor.execute("""
                            SELECT br.BlockType, br.Identifier, br.StartToken, br.EndToken, um.UserMarkGuid
                            FROM BlockRange br
                            JOIN UserMark um ON br.UserMarkId = um.UserMarkId
                            ORDER BY br.BlockType, br.Identifier
                        """)
                        rows = src_cursor.fetchall()

                        for row in rows:
                            block_type, identifier, start_token, end_token, usermark_guid = row
                            new_usermark_id = usermark_guid_map.get(usermark_guid)

                            if not new_usermark_id:
                                print(f"‚ö†Ô∏è GUID non mapp√©: {usermark_guid}")
                                continue

                            try:
                                dest_cursor.execute("""
                                    SELECT 1 FROM BlockRange
                                    WHERE BlockType=? AND Identifier=? AND UserMarkId=?
                                    AND StartToken=? AND EndToken=?
                                """, (block_type, identifier, new_usermark_id, start_token, end_token))

                                if dest_cursor.fetchone():
                                    print(f"‚è© Existe d√©j√†: {row}")
                                    continue

                                dest_cursor.execute("""
                                    INSERT INTO BlockRange
                                    (BlockType, Identifier, StartToken, EndToken, UserMarkId)
                                    VALUES (?, ?, ?, ?, ?)
                                """, (block_type, identifier, start_token, end_token, new_usermark_id))

                                print(f"‚úÖ Insert√©: {row}")

                            except sqlite3.IntegrityError as e:
                                print(f"‚ùå Erreur int√©grit√©: {e}")
                                print(f"Ligne probl√©matique: {row}")
                                dest_cursor.execute("PRAGMA foreign_key_check")
                                print("Probl√®mes cl√©s √©trang√®res:", dest_cursor.fetchall())
                                return False

                except Exception as e:
                    print(f"‚ùå Erreur lors du traitement de {db_path}: {e}")
                    return False

            # ‚úÖ Apr√®s avoir trait√© les deux fichiers, on fait 1 seul commit
            try:
                dest_conn.commit()
                print(f"‚úÖ Commit global effectu√© apr√®s tous les fichiers")
            except Exception as e:
                print(f"‚ùå Erreur critique pendant commit final : {e}")
                return False

    except Exception as e:
        print(f"‚ùå Erreur critique g√©n√©rale dans merge_blockrange_from_two_sources : {e}")
        return False

    return True


def merge_inputfields(merged_db_path, file1_db, file2_db, location_id_map):
    print("\n[üîÅ PURGE + FUSION INPUTFIELD - REMAP COMPLET]")
    inserted_count = 0
    missing_count = 0

    # Lire toutes les donn√©es √† r√©ins√©rer
    all_rows = []  # [(source_db, old_loc_id, tag, value)]

    for db_path in [file1_db, file2_db]:
        try:
            with sqlite3.connect(db_path) as src_conn:
                src_cursor = src_conn.cursor()
                src_cursor.execute("SELECT LocationId, TextTag, Value FROM InputField")
                for loc_id, tag, value in src_cursor.fetchall():
                    value = value if value is not None else ''
                    all_rows.append((db_path, loc_id, tag, value))
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture InputField depuis {db_path}: {e}")
            continue

    with sqlite3.connect(merged_db_path, timeout=10, check_same_thread=False) as conn:
        cursor = conn.cursor()

        # Supprimer toutes les anciennes lignes fusionn√©es
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS MergeMapping_InputField (
                SourceDb TEXT,
                OldLocationId INTEGER,
                TextTag TEXT,
                Value TEXT,
                PRIMARY KEY (SourceDb, OldLocationId, TextTag)
            )
        """)
        print("üßπ Suppression des anciennes donn√©es InputField fusionn√©es...")
        cursor.execute("""
            DELETE FROM InputField
            WHERE (LocationId, TextTag) IN (
                SELECT LocationId, TextTag
                FROM MergeMapping_InputField
            )
        """)
        cursor.execute("DELETE FROM MergeMapping_InputField")

        # R√©insertion propre
        for db_path, loc_id, tag, value in all_rows:
            mapped_loc = location_id_map.get((db_path, loc_id))
            if mapped_loc is None:
                print(f"‚ùå LocationId {loc_id} (depuis {db_path}) non mapp√© ‚Äî ligne ignor√©e")
                missing_count += 1
                continue

            try:
                cursor.execute("""
                    INSERT INTO InputField (LocationId, TextTag, Value)
                    VALUES (?, ?, ?)
                """, (mapped_loc, tag, value))
                inserted_count += 1
                cursor.execute("""
                    INSERT INTO MergeMapping_InputField (SourceDb, OldLocationId, TextTag, Value)
                    VALUES (?, ?, ?, ?)
                """, (db_path, loc_id, tag, value))
            except sqlite3.IntegrityError as e:
                print(f"‚ùå Conflit √† l‚Äôinsertion (Loc={mapped_loc}, Tag={tag}): {e}")

        conn.commit()

    print("\n=== [INPUTFIELD FINAL] ===")
    print(f"‚úÖ Lignes r√©ins√©r√©es   : {inserted_count}")
    print(f"‚ùå LocationId manquants : {missing_count}")


def update_location_references(merged_db_path, location_replacements):
    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    for old_loc, new_loc in location_replacements.items():
        # üîÅ Mise √† jour Bookmark.LocationId
        try:
            cursor.execute("UPDATE Bookmark SET LocationId = ? WHERE LocationId = ?", (new_loc, old_loc))
            print(f"Bookmark LocationId mis √† jour: {old_loc} -> {new_loc}")
        except Exception as e:
            print(f"Erreur mise √† jour Bookmark LocationId {old_loc}: {e}")

        # üîÅ Mise √† jour Bookmark.PublicationLocationId avec conflit Slot
        try:
            cursor.execute("""
                SELECT BookmarkId, Slot FROM Bookmark
                WHERE PublicationLocationId = ?
            """, (old_loc,))
            rows = cursor.fetchall()

            for bookmark_id, slot in rows:
                cursor.execute("""
                    SELECT 1 FROM Bookmark
                    WHERE PublicationLocationId = ? AND Slot = ? AND BookmarkId != ?
                """, (new_loc, slot, bookmark_id))
                conflict = cursor.fetchone()

                if conflict:
                    print(f"‚ö†Ô∏è Mise √† jour ignor√©e pour Bookmark ID {bookmark_id} (conflit avec PublicationLocationId={new_loc}, Slot={slot})")
                else:
                    cursor.execute("""
                        UPDATE Bookmark
                        SET PublicationLocationId = ?
                        WHERE BookmarkId = ?
                    """, (new_loc, bookmark_id))
                    print(f"Bookmark PublicationLocationId mis √† jour: {old_loc} -> {new_loc} (BookmarkId {bookmark_id})")
        except Exception as e:
            print(f"Erreur s√©curis√©e mise √† jour PublicationLocationId {old_loc}: {e}")

        # üîÅ Mise √† jour PlaylistItemLocationMap s√©curis√©e
        try:
            cursor.execute("""
                SELECT PlaylistItemId FROM PlaylistItemLocationMap
                WHERE LocationId = ?
            """, (old_loc,))
            rows = cursor.fetchall()

            for playlist_item_id, in rows:
                cursor.execute("""
                    SELECT 1 FROM PlaylistItemLocationMap
                    WHERE PlaylistItemId = ? AND LocationId = ?
                """, (playlist_item_id, new_loc))
                conflict = cursor.fetchone()

                if conflict:
                    print(f"‚ö†Ô∏è Mise √† jour ignor√©e pour PlaylistItemLocationMap: ItemId={playlist_item_id}, conflit LocationId {new_loc}")
                else:
                    cursor.execute("""
                        UPDATE PlaylistItemLocationMap
                        SET LocationId = ?
                        WHERE PlaylistItemId = ? AND LocationId = ?
                    """, (new_loc, playlist_item_id, old_loc))
                    print(f"PlaylistItemLocationMap mis √† jour: ItemId={playlist_item_id}, LocationId {old_loc} -> {new_loc}")
        except Exception as e:
            print(f"Erreur mise √† jour PlaylistItemLocationMap pour {old_loc} -> {new_loc}: {e}")

    conn.commit()
    try:
        conn.close()
        print("üîö Connexion ferm√©e dans update_location_references()")
    except Exception as e:
        print(f"‚ùå ERREUR lors de conn.close() : {e}")


def merge_usermark_from_sources(merged_db_path, file1_db, file2_db, location_id_map):
    print("\n[FUSION USERMARK - ID√âMPOTENTE]")
    mapping = {}

    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    # Cr√©er la table de mapping
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MergeMapping_UserMark (
            SourceDb TEXT,
            OldUserMarkId INTEGER,
            NewUserMarkId INTEGER,
            PRIMARY KEY (SourceDb, OldUserMarkId)
        )
    """)

    # R√©cup√©rer le dernier UserMarkId existant
    cursor.execute("SELECT COALESCE(MAX(UserMarkId), 0) FROM UserMark")
    max_id = cursor.fetchone()[0]

    for db_path in [file1_db, file2_db]:
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("""
                SELECT UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version 
                FROM UserMark
            """)
            rows = src_cursor.fetchall()

            for old_um_id, color, loc_id, style, guid, version in rows:
                # V√©rifier si d√©j√† mapp√©
                cursor.execute("""
                    SELECT NewUserMarkId FROM MergeMapping_UserMark
                    WHERE SourceDb = ? AND OldUserMarkId = ?
                """, (db_path, old_um_id))
                res = cursor.fetchone()
                if res:
                    mapping[(db_path, old_um_id)] = res[0]
                    mapping[guid] = res[0]
                    continue

                # Appliquer mapping LocationId
                new_loc = location_id_map.get((db_path, loc_id), loc_id) if loc_id is not None else None

                # V√©rifier si le GUID existe d√©j√† et r√©cup√©rer toutes ses donn√©es
                cursor.execute("""
                    SELECT UserMarkId, ColorIndex, LocationId, StyleIndex, Version 
                    FROM UserMark WHERE UserMarkGuid = ?
                """, (guid,))
                existing = cursor.fetchone()

                if existing:
                    existing_id, existing_color, existing_loc, existing_style, existing_version = existing

                    # Si toutes les donn√©es sont identiques, r√©utiliser l'ID existant
                    if (color, new_loc, style, version) == (
                    existing_color, existing_loc, existing_style, existing_version):
                        new_um_id = existing_id
                        print(f"‚è© UserMark guid={guid} d√©j√† pr√©sent (identique), r√©utilis√© (ID={new_um_id})")
                    else:
                        # Donn√©es diff√©rentes - g√©n√©rer un nouveau GUID
                        new_guid = str(uuid.uuid4())
                        max_id += 1
                        new_um_id = max_id
                        cursor.execute("""
                            INSERT INTO UserMark (UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (new_um_id, color, new_loc, style, new_guid, version))
                        print(
                            f"‚ö†Ô∏è Conflit UserMark guid={guid}, nouvelle entr√©e cr√©√©e avec nouveau GUID (NewID={new_um_id})")
                else:
                    # Nouvel enregistrement
                    max_id += 1
                    new_um_id = max_id
                    cursor.execute("""
                        INSERT INTO UserMark (UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (new_um_id, color, new_loc, style, guid, version))
                    print(f"‚úÖ Insertion UserMark guid={guid}, NewID={new_um_id}")

                # Mise √† jour des mappings
                mapping[(db_path, old_um_id)] = new_um_id
                mapping[guid] = new_um_id
                cursor.execute("""
                    INSERT OR REPLACE INTO MergeMapping_UserMark (SourceDb, OldUserMarkId, NewUserMarkId)
                    VALUES (?, ?, ?)
                """, (db_path, old_um_id, new_um_id))

    conn.commit()
    conn.close()
    print("Fusion UserMark termin√©e (idempotente).")
    return mapping


def insert_usermark_if_needed(conn, usermark_tuple):
    """
    Ins√®re ou met √† jour un UserMark si besoin.
    usermark_tuple = (UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version)
    """
    (um_id, color, loc, style, guid, version) = usermark_tuple

    cur = conn.cursor()

    # V√©rifie s'il existe d√©j√† un UserMark avec ce GUID
    existing = cur.execute("""
        SELECT UserMarkId, ColorIndex, LocationId, StyleIndex, Version 
        FROM UserMark 
        WHERE UserMarkGuid = ?
    """, (guid,)).fetchone()

    if existing:
        existing_id, ex_color, ex_loc, ex_style, ex_version = existing

        if (ex_color, ex_loc, ex_style, ex_version) == (color, loc, style, version):
            # Identique -> rien √† faire
            print(f"‚è© UserMark guid={guid} d√©j√† pr√©sent et identique, insertion ignor√©e.")
            return

        # Sinon : on fait un UPDATE pour aligner
        print(f"‚ö†Ô∏è Conflit d√©tect√© pour UserMark guid={guid}. Mise √† jour des champs.")
        cur.execute("""
            UPDATE UserMark
            SET ColorIndex = ?, LocationId = ?, StyleIndex = ?, Version = ?
            WHERE UserMarkGuid = ?
        """, (color, loc, style, version, guid))
    else:
        # N'existe pas ‚Üí insertion
        try:
            cur.execute("""
                INSERT INTO UserMark (UserMarkId, ColorIndex, LocationId, StyleIndex, UserMarkGuid, Version)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (um_id, color, loc, style, guid, version))
            print(f"‚úÖ UserMark guid={guid} ins√©r√© avec ID={um_id}")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'insertion du UserMark guid={guid}: {e}")


def merge_location_from_sources(merged_db_path, file1_db, file2_db):
    """
    Fusionne les enregistrements de la table Location depuis file1 et file2
    dans la base fusionn√©e de fa√ßon idempotente.
    Retourne un dictionnaire {(source_db, old_id) : new_id}.
    """
    print("\n[FUSION LOCATION - ID√âMPOTENTE]")

    def read_locations(db_path):
        with sqlite3.connect(db_path) as conn:
            cur = conn.cursor()
            cur.execute("""
                SELECT LocationId, BookNumber, ChapterNumber, DocumentId, Track,
                       IssueTagNumber, KeySymbol, MepsLanguage, Type, Title
                FROM Location
            """)
            return [(db_path,) + row for row in cur.fetchall()]

    # Lecture combin√©e des deux fichiers
    locations = read_locations(file1_db) + read_locations(file2_db)

    # Connexion √† la base fusionn√©e
    with sqlite3.connect(merged_db_path) as conn:
        cur = conn.cursor()

        # Cr√©er la table de mapping MergeMapping_Location si absente
        cur.execute("""
            CREATE TABLE IF NOT EXISTS MergeMapping_Location (
                SourceDb TEXT,
                OldID INTEGER,
                NewID INTEGER,
                PRIMARY KEY (SourceDb, OldID)
            )
        """)
        conn.commit()

        # R√©cup√©rer le plus grand LocationId existant
        cur.execute("SELECT COALESCE(MAX(LocationId), 0) FROM Location")
        current_max_id = cur.fetchone()[0]

        location_id_map = {}

        for entry in locations:
            db_source, old_loc_id, book_num, chap_num, doc_id, track, issue, key_sym, meps_lang, loc_type, title = entry

            # V√©rifie si ce Location a d√©j√† √©t√© fusionn√©
            cur.execute("""
                SELECT NewID FROM MergeMapping_Location
                WHERE SourceDb = ? AND OldID = ?
            """, (db_source, old_loc_id))
            res = cur.fetchone()
            if res:
                new_id = res[0]
                print(f"‚è© Location d√©j√† fusionn√©e OldID={old_loc_id} ‚Üí NewID={new_id} (Source: {db_source})")
                location_id_map[(db_source, old_loc_id)] = new_id
                continue

            # Recherche d'une correspondance exacte (m√™me contenu) avec gestion des NULL
            cur.execute("""
                SELECT LocationId FROM Location
                WHERE 
                    BookNumber IS ? AND
                    ChapterNumber IS ? AND
                    DocumentId IS ? AND
                    Track IS ? AND
                    IssueTagNumber = ? AND
                    KeySymbol IS ? AND
                    MepsLanguage IS ? AND
                    Type = ? AND
                    Title IS ?
            """, (book_num, chap_num, doc_id, track, issue, key_sym, meps_lang, loc_type, title))

            existing = cur.fetchone()

            if existing:
                new_id = existing[0]
                print(f"üîé Location existante trouv√©e OldID={old_loc_id} ‚Üí NewID={new_id} (Source: {db_source})")
            else:
                # Pas trouv√©e ‚Üí insertion
                current_max_id += 1
                new_id = current_max_id
                try:
                    cur.execute("""
                        INSERT INTO Location
                        (LocationId, BookNumber, ChapterNumber, DocumentId, Track,
                         IssueTagNumber, KeySymbol, MepsLanguage, Type, Title)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (new_id, book_num, chap_num, doc_id, track, issue, key_sym, meps_lang, loc_type, title))
                    print(f"‚úÖ Location ins√©r√©e : NewID={new_id} (Source: {db_source})")
                except sqlite3.IntegrityError as e:
                    print(f"‚ùå Erreur insertion Location OldID={old_loc_id}: {e}")
                    continue

            # Mapping ajout√©
            location_id_map[(db_source, old_loc_id)] = new_id

            # Ajout du mapping dans MergeMapping_Location
            cur.execute("""
                INSERT OR IGNORE INTO MergeMapping_Location (SourceDb, OldID, NewID)
                VALUES (?, ?, ?)
            """, (db_source, old_loc_id, new_id))

        conn.commit()

    print("‚úî Fusion Location termin√©e.")
    return location_id_map



@app.route('/upload', methods=['GET', 'POST'])
def upload_files():
    if request.method == 'GET':
        response = jsonify({"message": "Route /upload fonctionne (GET) !"})
        response.headers.add("Access-Control-Allow-Origin", "*")
        return response, 200

    if 'file1' not in request.files or 'file2' not in request.files:
        return jsonify({"error": "Veuillez envoyer deux fichiers userData.db"}), 400

    file1 = request.files['file1']
    file2 = request.files['file2']

    # D√©finir les dossiers d'extraction (o√π sera plac√© chaque fichier userData.db)
    extracted1 = os.path.join("extracted", "file1_extracted")
    extracted2 = os.path.join("extracted", "file2_extracted")

    os.makedirs(extracted1, exist_ok=True)
    os.makedirs(extracted2, exist_ok=True)

    # Supprimer les anciens fichiers s'ils existent
    file1_path = os.path.join(extracted1, "userData.db")
    file2_path = os.path.join(extracted2, "userData.db")

    if os.path.exists(file1_path):
        os.remove(file1_path)
    if os.path.exists(file2_path):
        os.remove(file2_path)

    # Sauvegarde des fichiers userData.db
    file1.save(file1_path)
    file2.save(file2_path)

    response = jsonify({"message": "Fichiers userData.db re√ßus et enregistr√©s avec succ√®s !"})
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response, 200


@app.route('/analyze', methods=['GET'])
def validate_db_path(db_path):
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"Database not found: {db_path}")


def analyze_files():
    try:
        file1_db = os.path.join(EXTRACT_FOLDER, "file1_extracted", "userData.db")
        file2_db = os.path.join(EXTRACT_FOLDER, "file2_extracted", "userData.db")

        validate_db_path(file1_db)  # Validation ajout√©e
        validate_db_path(file2_db)  # Validation ajout√©e
        data1 = read_notes_and_highlights(file1_db)
        data2 = read_notes_and_highlights(file2_db)

        response = jsonify({"file1": data1, "file2": data2})
        response.headers.add("Access-Control-Allow-Origin", "*")
        return response, 200  # ‚Üê MANQUAIT ICI

    except FileNotFoundError as e:
        return jsonify({"error": str(e)}), 404
    except Exception as e:
        app.logger.error(f"Analyze error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500


@app.route('/compare', methods=['GET'])
def compare_data():
    file1_db = os.path.join(EXTRACT_FOLDER, "file1_extracted", "userData.db")
    file2_db = os.path.join(EXTRACT_FOLDER, "file2_extracted", "userData.db")

    data1 = read_notes_and_highlights(file1_db)
    data2 = read_notes_and_highlights(file2_db)

    notes1 = {note[1]: note[2] for note in data1.get("notes", [])}
    notes2 = {note[1]: note[2] for note in data2.get("notes", [])}

    identical_notes = {}
    conflicts_notes = {}
    unique_notes_file1 = {}
    unique_notes_file2 = {}

    for title in set(notes1.keys()).intersection(set(notes2.keys())):
        if notes1[title] == notes2[title]:
            identical_notes[title] = notes1[title]
        else:
            conflicts_notes[title] = {"file1": notes1[title], "file2": notes2[title]}

    for title in set(notes1.keys()).difference(set(notes2.keys())):
        unique_notes_file1[title] = notes1[title]

    for title in set(notes2.keys()).difference(set(notes1.keys())):
        unique_notes_file2[title] = notes2[title]

    highlights1 = {h[1]: h[2] for h in data1.get("highlights", [])}
    highlights2 = {h[1]: h[2] for h in data2.get("highlights", [])}

    identical_highlights = {}
    conflicts_highlights = {}
    unique_highlights_file1 = {}
    unique_highlights_file2 = {}

    for loc in set(highlights1.keys()).intersection(set(highlights2.keys())):
        if highlights1[loc] == highlights2[loc]:
            identical_highlights[loc] = highlights1[loc]
        else:
            conflicts_highlights[loc] = {"file1": highlights1[loc], "file2": highlights2[loc]}

    for loc in set(highlights1.keys()).difference(set(highlights2.keys())):
        unique_highlights_file1[loc] = highlights1[loc]

    for loc in set(highlights2.keys()).difference(set(highights1.keys())):
        unique_highlights_file2[loc] = highlights2[loc]

    result = {
        "notes": {
            "identical": identical_notes,
            "conflicts": conflicts_notes,
            "unique_file1": unique_notes_file1,
            "unique_file2": unique_notes_file2
        },
        "highlights": {
            "identical": identical_highlights,
            "conflicts": conflicts_highlights,
            "unique_file1": unique_highlights_file1,
            "unique_file2": unique_highlights_file2
        }
    }

    response = jsonify(result)
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response, 200


def merge_tags_and_tagmap(merged_db_path, file1_db, file2_db, note_mapping, location_id_map, item_id_map):
    """
    Fusionne Tags et TagMap de fa√ßon idempotente et rapide.
    """
    print("\n[FUSION TAGS ET TAGMAP - ID√âMPOTENTE]")

    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MergeMapping_Tag (
            SourceDb TEXT,
            OldTagId INTEGER,
            NewTagId INTEGER,
            PRIMARY KEY (SourceDb, OldTagId)
        )
    """)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MergeMapping_TagMap (
            SourceDb TEXT,
            OldTagMapId INTEGER,
            NewTagMapId INTEGER,
            PRIMARY KEY (SourceDb, OldTagMapId)
        )
    """)
    conn.commit()

    # Fusion des Tags
    cursor.execute("SELECT COALESCE(MAX(TagId), 0) FROM Tag")
    max_tag_id = cursor.fetchone()[0]
    tag_id_map = {}

    for db_path in [file1_db, file2_db]:
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("SELECT TagId, Type, Name FROM Tag")
            for tag_id, tag_type, tag_name in src_cursor.fetchall():
                cursor.execute("SELECT NewTagId FROM MergeMapping_Tag WHERE SourceDb = ? AND OldTagId = ?", (db_path, tag_id))
                res = cursor.fetchone()
                if res:
                    tag_id_map[(db_path, tag_id)] = res[0]
                    continue

                cursor.execute("SELECT TagId FROM Tag WHERE Type = ? AND Name = ?", (tag_type, tag_name))
                existing = cursor.fetchone()
                if existing:
                    new_tag_id = existing[0]
                else:
                    max_tag_id += 1
                    new_tag_id = max_tag_id
                    cursor.execute("INSERT INTO Tag (TagId, Type, Name) VALUES (?, ?, ?)", (new_tag_id, tag_type, tag_name))

                tag_id_map[(db_path, tag_id)] = new_tag_id
                cursor.execute("INSERT INTO MergeMapping_Tag (SourceDb, OldTagId, NewTagId) VALUES (?, ?, ?)", (db_path, tag_id, new_tag_id))

    # Fusion des TagMap
    cursor.execute("SELECT COALESCE(MAX(TagMapId), 0) FROM TagMap")
    max_tagmap_id = cursor.fetchone()[0]
    tagmap_id_map = {}

    for db_path in [file1_db, file2_db]:
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("SELECT TagMapId, PlaylistItemId, LocationId, NoteId, TagId, Position FROM TagMap")
            rows = src_cursor.fetchall()

            for old_tagmap_id, playlist_item_id, location_id, note_id, old_tag_id, position in rows:
                new_tag_id = tag_id_map.get((db_path, old_tag_id))
                if new_tag_id is None:
                    continue

                new_note_id = note_mapping.get((db_path, note_id)) if note_id else None
                new_location_id = location_id_map.get((db_path, location_id)) if location_id else None
                new_playlist_item_id = item_id_map.get((db_path, playlist_item_id)) if playlist_item_id else None

                non_null_refs = sum(x is not None for x in [new_note_id, new_location_id, new_playlist_item_id])
                if non_null_refs != 1:
                    continue

                # V√©rification de doublon exact
                cursor.execute("""
                    SELECT TagMapId FROM TagMap
                    WHERE TagId = ?
                    AND IFNULL(PlaylistItemId, -1) = IFNULL(?, -1)
                    AND IFNULL(LocationId, -1) = IFNULL(?, -1)
                    AND IFNULL(NoteId, -1) = IFNULL(?, -1)
                    AND Position = ?
                """, (new_tag_id, new_playlist_item_id, new_location_id, new_note_id, position))
                if cursor.fetchone():
                    continue

                # V√©rification sp√©ciale sur (TagId, LocationId)
                if new_location_id is not None:
                    cursor.execute("""
                        SELECT TagMapId FROM TagMap
                        WHERE TagId = ? AND LocationId = ?
                    """, (new_tag_id, new_location_id))
                    existing = cursor.fetchone()
                    if existing:
                        tagmap_id_map[(db_path, old_tagmap_id)] = existing[0]
                        continue

                # Gestion de conflit sur (TagId, Position)
                tentative = position
                while True:
                    cursor.execute("SELECT 1 FROM TagMap WHERE TagId = ? AND Position = ?", (new_tag_id, tentative))
                    if not cursor.fetchone():
                        break
                    tentative += 1

                max_tagmap_id += 1
                new_tagmap_id = max_tagmap_id

                cursor.execute("""
                    INSERT INTO TagMap (TagMapId, PlaylistItemId, LocationId, NoteId, TagId, Position)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (new_tagmap_id, new_playlist_item_id, new_location_id, new_note_id, new_tag_id, tentative))

                cursor.execute("""
                    INSERT INTO MergeMapping_TagMap (SourceDb, OldTagMapId, NewTagMapId)
                    VALUES (?, ?, ?)
                """, (db_path, old_tagmap_id, new_tagmap_id))

                tagmap_id_map[(db_path, old_tagmap_id)] = new_tagmap_id

    conn.commit()
    conn.close()
    print("Fusion des Tags et TagMap termin√©e (idempotente).")
    return tag_id_map, tagmap_id_map


def merge_playlist_items(merged_db_path, file1_db, file2_db, im_mapping=None):
    """
    Fusionne PlaylistItem de fa√ßon idempotente.
    """
    print("\n[FUSION PLAYLISTITEMS - ID√âMPOTENTE]")

    mapping = {}
    conn = sqlite3.connect(merged_db_path, timeout=30)
    conn.execute("PRAGMA busy_timeout = 10000")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MergeMapping_PlaylistItem (
            SourceDb TEXT,
            OldItemId INTEGER,
            NewItemId INTEGER,
            PRIMARY KEY (SourceDb, OldItemId)
        )
    """)
    conn.commit()

    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='PlaylistItem'")
    if not cursor.fetchone():
        print("[ERREUR] La table PlaylistItem n'existe pas dans la DB fusionn√©e.")
        conn.close()
        return {}

    import hashlib
    def safe_text(val):
        return val if val is not None else ""

    def safe_number(val):
        return val if val is not None else 0

    def generate_full_key(label, start_trim, end_trim, accuracy, end_action, thumbnail_path):
        normalized = f"{label}|{start_trim}|{end_trim}|{accuracy}|{end_action}|{thumbnail_path}"
        return hashlib.sha256(normalized.encode("utf-8")).hexdigest()

    existing_items = {}

    def read_playlist_items(db_path):
        with sqlite3.connect(db_path) as src_conn:
            cur_source = src_conn.cursor()
            cur_source.execute("""
                SELECT PlaylistItemId, Label, StartTrimOffsetTicks, EndTrimOffsetTicks, Accuracy, EndAction, ThumbnailFilePath
                FROM PlaylistItem
            """)
            return [(db_path,) + row for row in cur_source.fetchall()]

    all_items = read_playlist_items(file1_db) + read_playlist_items(file2_db)
    print(f"Total playlist items lus : {len(all_items)}")

    for item in all_items:
        db_source = item[0]
        old_id, label, start_trim, end_trim, accuracy, end_action, thumb_path = item[1:]

        norm_label = safe_text(label)
        norm_start = safe_number(start_trim)
        norm_end = safe_number(end_trim)
        norm_thumb = safe_text(thumb_path)

        key = generate_full_key(norm_label, norm_start, norm_end, accuracy, end_action, norm_thumb)

        cursor.execute("SELECT NewItemId FROM MergeMapping_PlaylistItem WHERE SourceDb = ? AND OldItemId = ?", (db_source, old_id))
        res = cursor.fetchone()
        if res:
            new_id = res[0]
            mapping[(db_source, old_id)] = new_id
            if key not in existing_items:
                existing_items[key] = new_id
            continue

        if key in existing_items:
            new_id = existing_items[key]
        else:
            try:
                cursor.execute("""
                    INSERT INTO PlaylistItem 
                    (Label, StartTrimOffsetTicks, EndTrimOffsetTicks, Accuracy, EndAction, ThumbnailFilePath)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (label, start_trim, end_trim, accuracy, end_action, thumb_path))
                new_id = cursor.lastrowid
                existing_items[key] = new_id
            except sqlite3.IntegrityError as e:
                print(f"Erreur insertion PlaylistItem OldID {old_id} de {db_source}: {e}")
                continue

        cursor.execute("""
            INSERT INTO MergeMapping_PlaylistItem (SourceDb, OldItemId, NewItemId)
            VALUES (?, ?, ?)
        """, (db_source, old_id, new_id))
        mapping[(db_source, old_id)] = new_id

    conn.commit()
    conn.close()
    print(f"Total PlaylistItems mapp√©s: {len(mapping)}")
    return mapping


def merge_playlist_item_accuracy(merged_db_path, file1_db, file2_db):
    print("\n[FUSION PLAYLISTITEMACCURACY]")

    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    cursor.execute("SELECT COALESCE(MAX(PlaylistItemAccuracyId), 0) FROM PlaylistItemAccuracy")
    max_acc_id = cursor.fetchone()[0] or 0
    print(f"ID max initial: {max_acc_id}")

    for db_path in [file1_db, file2_db]:
        try:
            with sqlite3.connect(db_path) as src_conn:
                src_cursor = src_conn.cursor()
                src_cursor.execute("SELECT PlaylistItemAccuracyId, Description FROM PlaylistItemAccuracy")
                records = src_cursor.fetchall()
                print(f"{len(records)} records trouv√©s dans {os.path.basename(db_path)}")

                for acc_id, desc in records:
                    cursor.execute("""
                        INSERT OR IGNORE INTO PlaylistItemAccuracy (PlaylistItemAccuracyId, Description)
                        VALUES (?, ?)
                    """, (acc_id, desc))
                    max_acc_id = max(max_acc_id, acc_id)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du traitement de {db_path}: {e}")

    conn.commit()
    conn.close()
    print(f"ID max final: {max_acc_id}")
    return max_acc_id


def merge_playlist_item_location_map(merged_db_path, file1_db, file2_db, item_id_map, location_id_map):
    print("\n[FUSION PLAYLISTITEMLOCATIONMAP]")

    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    # √âtape 1: Vider compl√®tement la table
    cursor.execute("DELETE FROM PlaylistItemLocationMap")
    print("üóëÔ∏è Table PlaylistItemLocationMap vid√©e avant reconstruction")

    total_inserted = 0
    total_skipped = 0

    # √âtape 2: Reconstruction avec mapping
    for db_path in [file1_db, file2_db]:
        normalized_db = os.path.normpath(db_path)
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("""
                SELECT PlaylistItemId, LocationId, MajorMultimediaType, BaseDurationTicks
                FROM PlaylistItemLocationMap
            """)
            mappings = src_cursor.fetchall()
            print(f"{len(mappings)} mappings trouv√©s dans {os.path.basename(db_path)}")

            for old_item_id, old_loc_id, mm_type, duration in mappings:
                new_item_id = item_id_map.get((normalized_db, old_item_id))
                new_loc_id = location_id_map.get((normalized_db, old_loc_id))

                if new_item_id is None or new_loc_id is None:
                    print(f"‚ö†Ô∏è Ignor√©: PlaylistItemId={old_item_id} ou LocationId={old_loc_id} non mapp√© (source: {os.path.basename(db_path)})")
                    total_skipped += 1
                    continue

                try:
                    cursor.execute("""
                        INSERT INTO PlaylistItemLocationMap
                        (PlaylistItemId, LocationId, MajorMultimediaType, BaseDurationTicks)
                        VALUES (?, ?, ?, ?)
                    """, (new_item_id, new_loc_id, mm_type, duration))
                    print(f"‚úÖ Insertion: PlaylistItemId={new_item_id}, LocationId={new_loc_id}")
                    total_inserted += 1
                except sqlite3.IntegrityError as e:
                    print(f"‚ö†Ô∏è Doublon ignor√©: {e}")
                    total_skipped += 1

    conn.commit()

    print(f"üìä R√©sultat: {total_inserted} lignes ins√©r√©es, {total_skipped} ignor√©es")
    cursor.execute("SELECT COUNT(*) FROM PlaylistItemLocationMap")
    count = cursor.fetchone()[0]
    print(f"üîç Total final dans PlaylistItemLocationMap: {count} lignes")

    conn.close()


def cleanup_playlist_item_location_map(conn):
    cursor = conn.cursor()
    cursor.execute("""
        DELETE FROM PlaylistItemLocationMap
        WHERE PlaylistItemId NOT IN (
            SELECT PlaylistItemId FROM PlaylistItem
        )
    """)
    conn.commit()
    print("üßπ Nettoyage post-merge : PlaylistItemLocationMap nettoy√©e.")


def merge_playlist_item_independent_media_map(merged_db_path, file1_db, file2_db, item_id_map, independent_media_map):
    """
    Fusionne PlaylistItemIndependentMediaMap avec adaptation du mapping.
    """
    print("\n[FUSION PlaylistItemIndependentMediaMap]")
    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    # üßπ On vide la table avant de la reconstruire proprement
    cursor.execute("DELETE FROM PlaylistItemIndependentMediaMap")

    inserted = 0
    skipped = 0

    for db_path in [file1_db, file2_db]:
        normalized_db = os.path.normpath(db_path)
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("""
                SELECT PlaylistItemId, IndependentMediaId, DurationTicks
                FROM PlaylistItemIndependentMediaMap
            """)
            rows = src_cursor.fetchall()
            print(f"{len(rows)} lignes trouv√©es dans {os.path.basename(db_path)}")

            for old_item_id, old_media_id, duration_ticks in rows:
                new_item_id = item_id_map.get((normalized_db, old_item_id))
                new_media_id = independent_media_map.get((normalized_db, old_media_id))

                if new_item_id is None or new_media_id is None:
                    print(f"‚ö†Ô∏è Mapping manquant pour PlaylistItemId={old_item_id}, IndependentMediaId={old_media_id} (source: {normalized_db})")
                    skipped += 1
                    continue

                try:
                    cursor.execute("""
                        INSERT INTO PlaylistItemIndependentMediaMap
                        (PlaylistItemId, IndependentMediaId, DurationTicks)
                        VALUES (?, ?, ?)
                    """, (new_item_id, new_media_id, duration_ticks))
                    inserted += 1
                except sqlite3.IntegrityError as e:
                    print(f"üö´ Doublon ignor√© : {e}")
                    skipped += 1

    conn.commit()
    conn.close()
    print(f"‚úÖ PlaylistItemIndependentMediaMap : {inserted} ins√©r√©s, {skipped} ignor√©s.")


def merge_playlist_item_marker(merged_db_path, file1_db, file2_db, item_id_map):
    """
    Fusionne la table PlaylistItemMarker de fa√ßon idempotente.
    Retourne marker_id_map.
    """
    print("\n[FUSION PLAYLISTITEMMARKER]")
    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MergeMapping_PlaylistItemMarker (
            SourceDb TEXT,
            OldMarkerId INTEGER,
            NewMarkerId INTEGER,
            PRIMARY KEY (SourceDb, OldMarkerId)
        )
    """)
    conn.commit()

    cursor.execute("SELECT COALESCE(MAX(PlaylistItemMarkerId), 0) FROM PlaylistItemMarker")
    max_marker_id = cursor.fetchone()[0] or 0
    print(f"ID max initial: {max_marker_id}")
    marker_id_map = {}

    for db_path in [file1_db, file2_db]:
        normalized_db = os.path.normpath(db_path)
        with sqlite3.connect(db_path) as src_conn:
            src_cursor = src_conn.cursor()
            src_cursor.execute("""
                SELECT PlaylistItemMarkerId, PlaylistItemId, Label, StartTimeTicks, DurationTicks, EndTransitionDurationTicks
                FROM PlaylistItemMarker
            """)
            markers = src_cursor.fetchall()
            print(f"{len(markers)} markers trouv√©s dans {os.path.basename(db_path)}")

            for old_marker_id, old_item_id, label, start_time, duration, end_transition in markers:
                new_item_id = item_id_map.get((normalized_db, old_item_id))
                if not new_item_id:
                    print(f"    > ID item introuvable pour marker {old_marker_id} ‚Äî ignor√©")
                    continue

                # Utiliser la version normalis√©e aussi ici
                cursor.execute("""
                    SELECT NewMarkerId FROM MergeMapping_PlaylistItemMarker
                    WHERE SourceDb = ? AND OldMarkerId = ?
                """, (normalized_db, old_marker_id))
                res = cursor.fetchone()
                if res:
                    marker_id_map[(normalized_db, old_marker_id)] = res[0]
                    continue

                max_marker_id += 1
                new_row = (max_marker_id, new_item_id, label, start_time, duration, end_transition)

                try:
                    cursor.execute("""
                        INSERT INTO PlaylistItemMarker
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, new_row)
                    marker_id_map[(normalized_db, old_marker_id)] = max_marker_id
                    cursor.execute("""
                        INSERT INTO MergeMapping_PlaylistItemMarker (SourceDb, OldMarkerId, NewMarkerId)
                        VALUES (?, ?, ?)
                    """, (normalized_db, old_marker_id, max_marker_id))
                    conn.commit()
                except sqlite3.IntegrityError as e:
                    print(f"üö´ Erreur insertion PlaylistItemMarker pour OldMarkerId {old_marker_id}: {e}")

    print(f"ID max final: {max_marker_id}")
    print(f"Total markers mapp√©s: {len(marker_id_map)}")
    conn.close()
    return marker_id_map


def merge_marker_maps(merged_db_path, file1_db, file2_db, marker_id_map):
    """
    Fusionne les tables de mapping li√©es aux markers, y compris PlaylistItemMarkerBibleVerseMap
    et PlaylistItemMarkerParagraphMap.
    """
    print("\n[FUSION MARKER MAPS]")
    conn = sqlite3.connect(merged_db_path)
    cursor = conn.cursor()

    for map_type in ['BibleVerse', 'Paragraph']:
        table_name = f'PlaylistItemMarker{map_type}Map'
        cursor.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'")
        if not cursor.fetchone():
            print(f"Table {table_name} non trouv√©e - ignor√©e")
            continue
        print(f"\nFusion de {table_name}")

        for db_path in [file1_db, file2_db]:
            normalized_db = os.path.normpath(db_path)
            with sqlite3.connect(db_path) as src_conn:
                src_cursor = src_conn.cursor()
                src_cursor.execute(f"SELECT * FROM {table_name}")
                rows = src_cursor.fetchall()
                print(f"{len(rows)} entr√©es trouv√©es dans {os.path.basename(db_path)} pour {table_name}")

                for row in rows:
                    old_marker_id = row[0]
                    new_marker_id = marker_id_map.get((normalized_db, old_marker_id))
                    if not new_marker_id:
                        continue
                    new_row = (new_marker_id,) + row[1:]
                    placeholders = ",".join(["?"] * len(new_row))
                    try:
                        cursor.execute(f"INSERT OR IGNORE INTO {table_name} VALUES ({placeholders})", new_row)
                    except sqlite3.IntegrityError as e:
                        print(f"Erreur dans {table_name}: {e}")

    conn.commit()
    conn.close()


def merge_playlists(merged_db_path, file1_db, file2_db, location_id_map, independent_media_map, item_id_map):
    """Fusionne toutes les tables li√©es aux playlists en respectant les contraintes."""
    print("\n=== D√âBUT FUSION PLAYLISTS ===")

    file1_db = os.path.normpath(file1_db)
    file2_db = os.path.normpath(file2_db)

    max_media_id = 0  # üîß Ajout essentiel
    max_playlist_id = 0  # üîß pour √©viter 'not associated with a value'
    conn = None  # üß∑ Pour pouvoir le fermer plus tard

    try:
        conn = sqlite3.connect(merged_db_path, timeout=30)
        conn.execute("PRAGMA busy_timeout = 10000")
        cursor = conn.cursor()

        print("\n[INITIALISATION]")
        print(f"Base de fusion: {merged_db_path}")
        print(f"Source 1: {file1_db}")
        print(f"Source 2: {file2_db}")
        print(f"Location IDs mapp√©s: {len(location_id_map)}")

        item_id_map = merge_playlist_items(
            merged_db_path, file1_db, file2_db, independent_media_map  # ‚úÖ 4 max
        )

        # Appel imm√©diat √† merge_playlist_items pour avoir item_id_map dispo d√®s le d√©but
        print(f"Mapping PlaylistItems: {item_id_map}")

        # ... (la suite continue normalement)

        marker_id_map = {}

        # 1. Fusion de PlaylistItemAccuracy
        max_acc_id = merge_playlist_item_accuracy(merged_db_path, file1_db, file2_db)
        print(f"--> PlaylistItemAccuracy fusionn√©e, max ID final: {max_acc_id}")

        # 2. Fusion PlaylistItemMarker
        # Fusion de PlaylistItemMarker et r√©cup√©ration du mapping des markers
        marker_id_map = merge_playlist_item_marker(merged_db_path, file1_db, file2_db, item_id_map)
        print(f"--> PlaylistItemMarker fusionn√©e, markers mapp√©s: {len(marker_id_map)}")

        # 3. Fusion des PlaylistItemMarkerMap et Marker*Map (BibleVerse/Paragraph)
        print("\n[FUSION MARKER MAPS]")

        # 4. Fusion des PlaylistItemMarkerBibleVerseMap et ParagraphMap
        # Fusion des MarkerMaps (BibleVerse, Paragraph, etc.)
        merge_marker_maps(merged_db_path, file1_db, file2_db, marker_id_map)
        print("--> MarkerMaps fusionn√©es.")

        # 5. Fusion de PlaylistItemIndependentMediaMap
        # Fusion de PlaylistItemIndependentMediaMap (bas√©e sur PlaylistItemIndependentMediaMap)
        merge_playlist_item_independent_media_map(merged_db_path, file1_db, file2_db, item_id_map, independent_media_map)
        print("--> PlaylistItemIndependentMediaMap fusionn√©e.")

        # 6. Fusion PlaylistItemLocationMap
        merge_playlist_item_location_map(merged_db_path, file1_db, file2_db, item_id_map, location_id_map)
        print("--> PlaylistItemLocationMap fusionn√©e.")

        # Nettoyage : retirer les mappings avec des PlaylistItemId fant√¥mes
        with sqlite3.connect(merged_db_path) as conn:
            cleanup_playlist_item_location_map(conn)

        # ========================
        # Maintenant, on d√©marre les op√©rations qui ouvrent leurs propres connexions
        # ========================

        # 7. Fusion de la table IndependentMedia (am√©lior√©e)
        print("\n[FUSION INDEPENDENTMEDIA]")
        # On r√©utilise le mapping d√©j√† pr√©par√© dans merge_data
        im_mapping = independent_media_map

        # 8. V√©rification finale des thumbnails
        print("\n[V√âRIFICATION THUMBNAILS ORPHELINS]")
        cursor.execute("""
            SELECT p.PlaylistItemId, p.ThumbnailFilePath
            FROM PlaylistItem p
            WHERE p.ThumbnailFilePath IS NOT NULL
              AND NOT EXISTS (
                  SELECT 1 FROM IndependentMedia m 
                  WHERE m.FilePath = p.ThumbnailFilePath
              )
        """)
        orphaned_thumbnails = cursor.fetchall()
        if orphaned_thumbnails:
            print(f"Avertissement : {len(orphaned_thumbnails)} thumbnails sans m√©dia associ√©")

            # ‚úÖ Ajoute ceci ici (pas en dehors)
            conn.commit()

        # 9. Finalisation playlists
        print("\n=== FUSION PLAYLISTS TERMIN√âE ===")
        playlist_results = {
            'item_id_map': item_id_map,
            'marker_id_map': marker_id_map,
            'media_status': {
                'total_media': max_media_id,
                'orphaned_thumbnails': len(orphaned_thumbnails) if 'orphaned_thumbnails' in locals() else 0
            }
        }
        print(f"R√©sum√© interm√©diaire: {playlist_results}")

        # 10. Finalisation
        # commit final et fermeture propre
        conn.commit()

        # üîö Fin de merge_playlists (retour principal)
        orphaned_deleted = 0  # ou remplace par la vraie valeur si elle est calcul√©e plus haut
        playlist_item_total = len(item_id_map)

        print("\nüß™ DEBUG FINAL DANS merge_playlists")
        print("Item ID Map complet:")
        for (src, old_id), new_id in item_id_map.items():
            print(f"  {src} ‚Äî {old_id} ‚Üí {new_id}")

        with sqlite3.connect(merged_db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("PRAGMA quick_check")
            integrity_result = cursor.fetchone()[0]

        return (
            max_playlist_id,
            len(item_id_map),
            max_media_id,
            orphaned_deleted,
            integrity_result,
            item_id_map
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ERREUR CRITIQUE dans merge_playlists: {str(e)}")
        return None, 0, 0, 0, "error", {}

    finally:
        if conn:
            try:
                conn.close()
            except:
                pass


def check_duplicate_guids_between_sources(file1_db, file2_db):
    """V√©rifie s'il y a des GUIDs en commun entre les deux sources"""
    guids_file1 = set()
    guids_file2 = set()

    with sqlite3.connect(file1_db) as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT UserMarkGuid FROM UserMark")
        guids_file1 = {row[0] for row in cursor.fetchall()}

    with sqlite3.connect(file2_db) as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT UserMarkGuid FROM UserMark")
        guids_file2 = {row[0] for row in cursor.fetchall()}

    return guids_file1 & guids_file2


def create_note_mapping(merged_db_path, file1_db, file2_db):
    """Cr√©e un mapping (source_db_path, old_note_id) -> new_note_id en se basant sur les GUID."""
    mapping = {}
    try:
        with sqlite3.connect(merged_db_path, timeout=30) as merged_conn:
            merged_conn.execute("PRAGMA busy_timeout = 10000")
            merged_cursor = merged_conn.cursor()
            merged_cursor.execute("SELECT Guid, NoteId FROM Note")
            merged_guid_map = {guid: note_id for guid, note_id in merged_cursor.fetchall() if guid}

        for db_path in [file1_db, file2_db]:
            if not os.path.exists(db_path):
                print(f"[WARN] Fichier DB manquant : {db_path}")
                continue

            with sqlite3.connect(db_path) as src_conn:
                src_cursor = src_conn.cursor()
                src_cursor.execute("SELECT NoteId, Guid FROM Note")
                for old_note_id, guid in src_cursor.fetchall():
                    if guid and guid in merged_guid_map:
                        mapping[(db_path, old_note_id)] = merged_guid_map[guid]

    except Exception as e:
        print(f"[ERREUR] create_note_mapping: {str(e)}")

    return mapping or {}


def merge_platform_metadata(merged_db_path, db1_path, db2_path):
    print("üîß Fusion combin√©e android_metadata + grdb_migrations")
    locales = set()
    identifiers = set()

    for db_path in [db1_path, db2_path]:
        with sqlite3.connect(db_path) as src:
            cursor = src.cursor()
            try:
                cursor.execute("SELECT locale FROM android_metadata")
                locales.update(row[0] for row in cursor.fetchall())
            except sqlite3.OperationalError:
                print(f"‚ÑπÔ∏è Table android_metadata absente de {db_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lecture android_metadata depuis {db_path}: {e}")

            try:
                cursor.execute("SELECT identifier FROM grdb_migrations")
                identifiers.update(row[0] for row in cursor.fetchall())
            except sqlite3.OperationalError:
                print(f"‚ÑπÔ∏è Table grdb_migrations absente de {db_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lecture grdb_migrations depuis {db_path}: {e}")

    # Une seule connexion d‚Äô√©criture pour les deux insertions
    with sqlite3.connect(merged_db_path, timeout=15) as conn:
        cursor = conn.cursor()

        if locales:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS android_metadata (
                    locale TEXT
                )
            """)
            cursor.execute("DELETE FROM android_metadata")
            for loc in locales:
                print(f"‚úÖ INSERT android_metadata.locale = {loc}")
                cursor.execute("INSERT INTO android_metadata (locale) VALUES (?)", (loc,))

        if identifiers:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS grdb_migrations (
                    identifier TEXT NOT NULL PRIMARY KEY
                )
            """)
            cursor.execute("DELETE FROM grdb_migrations")
            for ident in sorted(identifiers):
                print(f"‚úÖ INSERT grdb_migrations.identifier = {ident}")
                cursor.execute("INSERT INTO grdb_migrations (identifier) VALUES (?)", (ident,))


@app.route('/merge', methods=['POST'])
def merge_data():
    start_time = time.time()
    # Au tout d√©but du merge
    open(os.path.join(UPLOAD_FOLDER, "merge_in_progress"), "w").close()

    # ‚îÄ‚îÄ‚îÄ 0. Initialisation des variables utilis√©es plus bas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    merged_jwlibrary = None
    max_playlist_id = 0
    max_media_id = 0
    orphaned_deleted = 0
    integrity_result = "ok"
    item_id_map = {}
    marker_id_map = {}
    playlist_id_map = {}

    conn = None  # pour le finally

    try:
        global note_mapping  # Si vous souhaitez utiliser le scope global (optionnel)
        payload = request.get_json()
        conflict_choices_notes = payload.get("conflicts_notes", {})
        conflict_choices_highlights = payload.get("conflicts_highlights", {})
        local_datetime = payload.get("local_datetime")
        print(f"local_datetime re√ßu du client : {local_datetime}")
        if local_datetime:
            merge_date = local_datetime if len(local_datetime) > 16 else local_datetime + ":00"
        else:
            merge_date = get_current_local_iso8601()

        file1_db = os.path.join(EXTRACT_FOLDER, "file1_extracted", "userData.db")
        file2_db = os.path.join(EXTRACT_FOLDER, "file2_extracted", "userData.db")

        # Validation fichiers sources
        if not all(os.path.exists(db) for db in [file1_db, file2_db]):
            return jsonify({"error": "Fichiers source manquants"}), 400

        data1 = read_notes_and_highlights(file1_db)
        data2 = read_notes_and_highlights(file2_db)

        highlights_db1 = data1["highlights"]
        highlights_db2 = data2["highlights"]
        merged_highlights_dict = {}
        for h in highlights_db1:
            _, color, loc, style, guid, version = h
            merged_highlights_dict[guid] = (color, loc, style, version)
        for h in highlights_db2:
            _, color2, loc2, style2, guid2, version2 = h
            if guid2 not in merged_highlights_dict:
                merged_highlights_dict[guid2] = (color2, loc2, style2, version2)
            else:
                (color1, loc1, style1, version1) = merged_highlights_dict[guid2]
                if (color1 == color2 and loc1 == loc2 and style1 == style2 and version1 == version2):
                    continue
                else:
                    choice = conflict_choices_highlights.get(guid2, "file1")
                    if choice == "file2":
                        merged_highlights_dict[guid2] = (color2, loc2, style2, version2)

        # === Validation pr√©alable ===
        required_dbs = [
            os.path.join(EXTRACT_FOLDER, "file1_extracted", "userData.db"),
            os.path.join(EXTRACT_FOLDER, "file2_extracted", "userData.db")
        ]
        if not all(os.path.exists(db) for db in required_dbs):
            return jsonify({"error": "Fichiers source manquants"}), 400

        # Cr√©ation de la DB fusionn√©e
        merged_db_path = os.path.join(UPLOAD_FOLDER, "merged_userData.db")
        if os.path.exists(merged_db_path):
            os.remove(merged_db_path)
        base_db_path = os.path.join(EXTRACT_FOLDER, "file1_extracted", "userData.db")
        create_merged_schema(merged_db_path, base_db_path)

        # juste apr√®s create_merged_schema(merged_db_path, base_db_path)
        print("\n‚Üí Debug: listing des tables juste apr√®s create_merged_schema")
        with sqlite3.connect(merged_db_path) as dbg_conn:
            dbg_cur = dbg_conn.cursor()
            dbg_cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [t[0] for t in dbg_cur.fetchall()]
            print("Tables pr√©sentes dans merged_userData.db :", tables)

        try:
            location_id_map = merge_location_from_sources(merged_db_path, *required_dbs)
            print("Location ID Map:", location_id_map)
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_location_from_sources : {e}")
            traceback.print_exc()
            raise

        try:
            independent_media_map = merge_independent_media(merged_db_path, file1_db, file2_db)
            print("Mapping IndependentMedia:", independent_media_map)
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_independent_media : {e}")
            traceback.print_exc()
            raise

        # ‚ùå NE PAS appeler merge_playlist_items ici
        # item_id_map = merge_playlist_items(...)

        common_guids = check_duplicate_guids_between_sources(file1_db, file2_db)
        if common_guids:
            print(f"‚ö†Ô∏è Attention: {len(common_guids)} GUIDs en commun entre les deux sources")
            # Afficher les 5 premiers pour le debug
            for guid in list(common_guids)[:5]:
                print(f"- {guid}")
        else:
            print("‚úÖ Aucun GUID en commun entre les deux sources")

        try:
            usermark_guid_map = merge_usermark_from_sources(merged_db_path, file1_db, file2_db, location_id_map)

        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_usermark_from_sources : {e}")
            traceback.print_exc()
            raise

        # Apr√®s le bloc try/except de merge_usermark_from_sources
        with sqlite3.connect(merged_db_path) as conn:
            cursor = conn.cursor()
            # V√©rifier les doublons potentiels
            cursor.execute("""
                SELECT UserMarkGuid, COUNT(*) as cnt 
                FROM UserMark 
                GROUP BY UserMarkGuid 
                HAVING cnt > 1
            """)
            duplicates = cursor.fetchall()
            if duplicates:
                print("‚ö†Ô∏è Attention: GUIDs dupliqu√©s d√©tect√©s apr√®s fusion:")
                for guid, count in duplicates:
                    print(f"- {guid}: {count} occurrences")
            else:
                print("‚úÖ Aucun GUID dupliqu√© d√©tect√© apr√®s fusion")

        # Gestion sp√©cifique de LastModified
        conn = sqlite3.connect(merged_db_path)
        cursor = conn.cursor()

        cursor.execute("DELETE FROM LastModified")
        cursor.execute("INSERT INTO LastModified (LastModified) VALUES (?)", (merge_date,))
        conn.commit()
        conn.close()

        try:
            note_mapping = create_note_mapping(merged_db_path, file1_db, file2_db)
            print("Note Mapping:", note_mapping)
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans create_note_mapping : {e}")
            traceback.print_exc()
            raise

        # (R√©)ouvrir la connexion pour PlaylistItem
        conn = sqlite3.connect(merged_db_path)
        cursor = conn.cursor()

        print(f"--> PlaylistItem fusionn√©s : {len(item_id_map)} items")

        conn.close()

        print("\n=== USERMARK VERIFICATION ===")
        print(f"Total UserMarks mapp√©s (GUIDs) : {len(usermark_guid_map)}")
        with sqlite3.connect(merged_db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM UserMark")
            total = cursor.fetchone()[0]
            print(f"UserMarks dans la DB: {total}")
            cursor.execute("""
                SELECT ColorIndex, StyleIndex, COUNT(*) 
                FROM UserMark 
                GROUP BY ColorIndex, StyleIndex
            """)
            print("R√©partition par couleur/style:")
            for color, style, count in cursor.fetchall():
                print(f"- Couleur {color}, Style {style}: {count} marques")

        print(f"Location IDs mapp√©s: {location_id_map}")
        print(f"UserMark GUIDs mapp√©s: {usermark_guid_map}")

        # ===== V√©rification pr√©-fusion compl√®te =====
        print("\n=== VERIFICATION PRE-FUSION ===")
        print("\n[V√âRIFICATION FICHIERS SOURCES]")
        source_files = [
            os.path.join(EXTRACT_FOLDER, "file1_extracted", "userData.db"),
            os.path.join(EXTRACT_FOLDER, "file2_extracted", "userData.db")
        ]
        for file in source_files:
            print(f"V√©rification {file}... ", end="")
            if not os.path.exists(file):
                print("ERREUR: Fichier manquant")
                return jsonify({"error": f"Fichier source manquant: {file}"}), 400
            else:
                print(f"OK ({os.path.getsize(file) / 1024:.1f} KB)")

        print("\n[V√âRIFICATION SCH√âMA]")

        def verify_schema(db_path):
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [t[0] for t in cursor.fetchall()]
                print(f"Tables dans {os.path.basename(db_path)}: {len(tables)}")
                required_tables = {'Bookmark', 'Location', 'UserMark', 'Note'}
                missing = required_tables - set(tables)
                if missing:
                    print(f"  TABLES MANQUANTES: {missing}")
                conn.close()
                return not bool(missing)
            except Exception as e:
                print(f"  ERREUR: {str(e)}")
                return False
        if not all(verify_schema(db) for db in source_files):
            return jsonify({"error": "Sch√©ma de base de donn√©es incompatible"}), 400

        print("\n[V√âRIFICATION BASE DE DESTINATION]")
        print(f"V√©rification {merged_db_path}... ", end="")
        try:
            conn = sqlite3.connect(merged_db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [t[0] for t in cursor.fetchall()]
            print(f"OK ({len(tables)} tables)")
            conn.close()
        except Exception as e:
            print(f"ERREUR: {str(e)}")
            return jsonify({"error": "Base de destination corrompue"}), 500

        print("\n[V√âRIFICATION SYST√àME]")
        try:
            import psutil
            mem = psutil.virtual_memory()
            print(f"M√©moire disponible: {mem.available / 1024 / 1024:.1f} MB")
            if mem.available < 500 * 1024 * 1024:
                print("ATTENTION: M√©moire insuffisante")
        except ImportError:
            print("psutil non install√© - v√©rification m√©moire ignor√©e")

        print("\n=== PR√äT POUR FUSION ===\n")

        try:
            merge_bookmarks(merged_db_path, file1_db, file2_db, location_id_map)
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_bookmarks : {e}")
            traceback.print_exc()
            raise

        # --- FUSION BLOCKRANGE ---
        print("\n=== DEBUT FUSION BLOCKRANGE ===")
        try:
            if not merge_blockrange_from_two_sources(merged_db_path, file1_db, file2_db):
                print("√âCHEC Fusion BlockRange")
                return jsonify({"error": "BlockRange merge failed"}), 500
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_blockrange_from_two_sources : {e}")
            traceback.print_exc()
            raise

        # Mapping inverse UserMarkId original ‚Üí nouveau
        usermark_guid_map = {}
        conn = sqlite3.connect(merged_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT UserMarkId, UserMarkGuid FROM UserMark")
        for new_id, guid in cursor.fetchall():
            usermark_guid_map[guid] = new_id
        conn.close()

        try:
            note_mapping = merge_notes(
                merged_db_path,
                file1_db,
                file2_db,
                location_id_map,
                usermark_guid_map
            )
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_notes : {e}")
            traceback.print_exc()
            raise

        # --- √âtape suivante : fusion des Tags et TagMap ---
        try:
            tag_id_map, tagmap_id_map = merge_tags_and_tagmap(
                merged_db_path,
                file1_db,
                file2_db,
                note_mapping,
                location_id_map,
                item_id_map
            )
            print(f"Tag ID Map: {tag_id_map}")
            print(f"TagMap ID Map: {tagmap_id_map}")

        except Exception as e:
            import traceback
            print("‚ùå √âchec de merge_tags_and_tagmap (mais on continue le merge global) :")
            print(f"Exception captur√©e : {e}")
            traceback.print_exc()
            tag_id_map, tagmap_id_map = {}, {}

        # --- V√©rification Tag ---
        print("\n=== TAGS VERIFICATION ===")

        try:
            with sqlite3.connect(merged_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM Tag")
                tags_count = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM TagMap")
                tagmaps_count = cursor.fetchone()[0]
                print(f"Tags: {tags_count}")
                print(f"TagMaps: {tagmaps_count}")
                cursor.execute("""
                    SELECT COUNT(*) 
                    FROM TagMap 
                    WHERE NoteId NOT IN (SELECT NoteId FROM Note)
                """)
                orphaned = cursor.fetchone()[0]
                print(f"TagMaps orphelins: {orphaned}")
        except Exception as e:
            print(f"‚ùå ERREUR dans la v√©rification des tags : {e}")
            import traceback
            traceback.print_exc()
            return jsonify({"error": "Erreur lors de la v√©rification des tags"}), 500

        print("\n‚ñ∂Ô∏è D√©but de la fusion des √©l√©ments li√©s aux playlists...")

        print("\n‚ñ∂Ô∏è Fusion des √©l√©ments li√©s aux playlists termin√©e.")

        # ‚îÄ‚îÄ‚îÄ Avant merge_other_tables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tables_to_check = [
            'PlaylistItem',
            'IndependentMedia',
            'PlaylistItemLocationMap',
            'PlaylistItemIndependentMediaMap'
        ]
        print("\n--- COMPTES AVANT merge_other_tables ---")
        with sqlite3.connect(merged_db_path) as dbg_conn:
            dbg_cur = dbg_conn.cursor()
            for tbl in tables_to_check:
                dbg_cur.execute(f"SELECT COUNT(*) FROM {tbl}")
                cnt_merged = dbg_cur.fetchone()[0]

                dbg_cur.execute(f"ATTACH DATABASE ? AS src1", (file1_db,))
                dbg_cur.execute(f"SELECT COUNT(*) FROM src1.{tbl}")
                cnt1 = dbg_cur.fetchone()[0]
                dbg_cur.execute("DETACH DATABASE src1")

                dbg_cur.execute(f"ATTACH DATABASE ? AS src2", (file2_db,))
                dbg_cur.execute(f"SELECT COUNT(*) FROM src2.{tbl}")
                cnt2 = dbg_cur.fetchone()[0]
                dbg_cur.execute("DETACH DATABASE src2")

                print(f"[AVANT ] {tbl}: merged={cnt_merged}, file1={cnt1}, file2={cnt2}")

        # Fermer toutes les connexions avant les appels suivants
        try:
            merge_other_tables(
                merged_db_path,
                file1_db,
                file2_db,
                exclude_tables=[
                    'Note', 'UserMark', 'Location', 'BlockRange',
                    'LastModified', 'Tag', 'TagMap', 'PlaylistItem',
                    'InputField', 'Bookmark', 'android_metadata', 'grdb_migrations'
                ]
            )
        except Exception as e:
            import traceback
            print(f"‚ùå Erreur dans merge_other_tables : {e}")
            traceback.print_exc()
            raise

        merge_platform_metadata(merged_db_path, file1_db, file2_db)

        # ‚îÄ‚îÄ‚îÄ Apr√®s merge_other_tables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        print("\n--- COMPTES APR√àS merge_other_tables ---")
        with sqlite3.connect(merged_db_path) as dbg_conn:
            dbg_cur = dbg_conn.cursor()
            for tbl in tables_to_check:
                dbg_cur.execute(f"SELECT COUNT(*) FROM {tbl}")
                cnt_merged = dbg_cur.fetchone()[0]

                dbg_cur.execute(f"ATTACH DATABASE ? AS src1", (file1_db,))
                dbg_cur.execute(f"SELECT COUNT(*) FROM src1.{tbl}")
                cnt1 = dbg_cur.fetchone()[0]
                dbg_cur.execute("DETACH DATABASE src1")

                dbg_cur.execute(f"ATTACH DATABASE ? AS src2", (file2_db,))
                dbg_cur.execute(f"SELECT COUNT(*) FROM src2.{tbl}")
                cnt2 = dbg_cur.fetchone()[0]
                dbg_cur.execute("DETACH DATABASE src2")

                print(f"[APR√àS] {tbl}: merged={cnt_merged}, file1={cnt1}, file2={cnt2}")

        # 8. V√©rification finale des thumbnails
        print("\n[V√âRIFICATION THUMBNAILS ORPHELINS]")
        cursor.execute("""
                    SELECT p.PlaylistItemId, p.ThumbnailFilePath
                    FROM PlaylistItem p
                    WHERE p.ThumbnailFilePath IS NOT NULL
                      AND NOT EXISTS (
                          SELECT 1 FROM IndependentMedia m 
                          WHERE m.FilePath = p.ThumbnailFilePath
                      )
                """)
        orphaned_thumbnails = cursor.fetchall()
        if orphaned_thumbnails:
            print(f"Avertissement : {len(orphaned_thumbnails)} thumbnails sans m√©dia associ√©")

        # 9. Finalisation playlists
        print("\n=== FUSION PLAYLISTS TERMIN√âE ===")
        playlist_results = {
            'item_id_map': item_id_map,
            'marker_id_map': marker_id_map,
            'media_status': {
                'total_media': max_media_id,
                'orphaned_thumbnails': len(orphaned_thumbnails) if 'orphaned_thumbnails' in locals() else 0
            }
        }
        print(f"R√©sum√© interm√©diaire: {playlist_results}")

        # 11. V√©rification de coh√©rence
        print("\n=== VERIFICATION COHERENCE ===")
        cursor.execute("""
            SELECT COUNT(*) 
              FROM PlaylistItem pi
             WHERE pi.PlaylistItemId NOT IN (
                    SELECT PlaylistItemId FROM PlaylistItemLocationMap
                    UNION
                    SELECT PlaylistItemId FROM PlaylistItemIndependentMediaMap
                )
        """)
        orphaned_items = cursor.fetchone()[0]
        status_color = "\033[91m" if orphaned_items > 0 else "\033[92m"
        print(f"{status_color}√âl√©ments sans parent d√©tect√©s (non supprim√©s) : {orphaned_items}\033[0m")

        # 12. Suppression des PlaylistItem orphelins
        with sqlite3.connect(merged_db_path) as conn_del:
            cur = conn_del.cursor()
            cur.execute("""
                DELETE FROM PlaylistItem
                 WHERE PlaylistItemId NOT IN (
                    SELECT PlaylistItemId FROM PlaylistItemLocationMap
                    UNION
                    SELECT PlaylistItemId FROM PlaylistItemIndependentMediaMap
                 )
            """)
            conn_del.commit()
        print("‚Üí PlaylistItem orphelins supprim√©s")

        # 13. Optimisations finales
        print("\n=== DEBUT OPTIMISATIONS ===")

        # D√©finition de log_message **avant** son premier appel
        log_file = os.path.join(UPLOAD_FOLDER, "fusion.log")

        def log_message(message, log_type="INFO"):
            print(message)
            with open(log_file, "a") as f:
                f.write(f"[{log_type}] {datetime.now().strftime('%H:%M:%S')} - {message}\n")

        # 13.1 Reconstruction des index
        print("\nReconstruction des index...")
        cursor.execute("SELECT name FROM sqlite_master WHERE type='index'")
        indexes = [row[0] for row in cursor.fetchall() if not row[0].startswith('sqlite_autoindex_')]
        for index_name in indexes:
            try:
                cursor.execute(f"REINDEX {index_name}")
                log_message(f"Index reconstruit: {index_name}")
            except sqlite3.Error as e:
                log_message(f"ERREUR sur index {index_name}: {str(e)}", "ERROR")

        # 13.2 V√©rification int√©grit√©
        print("\nV√©rification int√©grit√© base de donn√©es...")
        cursor.execute("PRAGMA quick_check")
        integrity_result = cursor.fetchone()[0]
        if integrity_result == "ok":
            log_message("Int√©grit√© de la base: OK")
        else:
            log_message(f"ERREUR int√©grit√©: {integrity_result}", "ERROR")

        # 13.3 V√©rification cl√©s √©trang√®res
        cursor.execute("PRAGMA foreign_key_check")
        fk_issues = cursor.fetchall()
        if fk_issues:
            log_message(f"ATTENTION: {len(fk_issues)} probl√®mes de cl√©s √©trang√®res", "WARNING")
            for issue in fk_issues[:3]:
                log_message(f"- Probl√®me: {issue}", "WARNING")
        else:
            log_message("Aucun probl√®me de cl√© √©trang√®re d√©tect√©")

        # --- 14. Finalisation ---
        # commit final et fermeture propre de la transaction playlists
        conn.commit()

        # R√©capitulatif final
        print("\n=== R√âCAPITULATIF FINAL ===")
        print(f"{'Playlists:':<20}, {max_playlist_id}")
        print(f"{'√âl√©ments:':<20}, {len(item_id_map)}")
        print(f"{'M√©dias:':<20}, {max_media_id}")
        print(f"{'Nettoy√©s:':<20}, {orphaned_deleted}")
        print(f"{'Int√©grit√©:':<20}, {integrity_result}")
        if fk_issues:
            print(f"{'Probl√®mes FK:':<20} \033[91m{len(fk_issues)}\033[0m")
        else:
            print(f"{'Probl√®mes FK:':<20} \033[92mAucun\033[0m")

        # 16. Activation du WAL
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("CREATE TABLE IF NOT EXISTS dummy_for_wal (id INTEGER PRIMARY KEY)")
        cursor.execute("INSERT INTO dummy_for_wal DEFAULT VALUES")
        cursor.execute("DELETE FROM dummy_for_wal")
        cursor.execute("DROP TABLE dummy_for_wal")
        conn.commit()
        conn.close()

        # V√©rification du mode WAL
        with sqlite3.connect(merged_db_path) as test_conn:
            new_wal_status = test_conn.execute("PRAGMA journal_mode").fetchone()[0]
            print(f"Statut WAL apr√®s activation: {new_wal_status}")
            if new_wal_status != "wal":
                print("Avertissement: √âchec de l'activation WAL")

        print("üìç Avant le r√©sum√© final")

        print("‚ñ∂Ô∏è Appel de merge_playlists...")
        print("üõë merge_playlists appel√©e")

        try:
            result = merge_playlists(
                merged_db_path,
                file1_db,
                file2_db,
                location_id_map,
                independent_media_map,
                item_id_map  # ‚ö†Ô∏è on passe le dict d√©j√† d√©fini (pas un nouveau {})
            )

            # üîÑ mise √† jour propre des variables
            (
                max_playlist_id,
                playlist_item_total,
                max_media_id,
                orphaned_deleted,
                integrity_result,
                item_id_map
            ) = result

            print("\nüîç V√©rification sp√©cifique de item_id_map pour PlaylistItemId 1 et 2")

            for test_id in [1, 2]:
                for db in [file1_db, file2_db]:
                    key = (db, test_id)
                    found = item_id_map.get(key)
                    print(f"  {key} ‚Üí {found}")

            # üß™ R√©sum√© post merge_playlists
            print("\nüéØ R√©sum√© final apr√®s merge_playlists:")
            print(f"- Playlists max ID: {max_playlist_id}")
            print(f"- PlaylistItem total: {playlist_item_total}")
            print(f"- M√©dias max ID: {max_media_id}")
            print(f"- Orphelins supprim√©s: {orphaned_deleted}")
            print(f"- R√©sultat int√©grit√©: {integrity_result}")
            print("‚úÖ Tous les calculs termin√©s, nettoyage‚Ä¶")

            print("item_id_map keys:", list(item_id_map.keys()))
            print("location_id_map keys:", list(location_id_map.keys()))
            print("note_mapping keys:", list(note_mapping.keys()))

            print("üì¶ V√©rification compl√®te de item_id_map AVANT merge_tags_and_tagmap:")
            for (db_path, old_id), new_id in item_id_map.items():
                print(f"  FROM {db_path} - OldID: {old_id} ‚Üí NewID: {new_id}")

            print("üß™ CONTENU DE item_id_map APR√àS merge_playlists:")
            for k, v in item_id_map.items():
                print(f"  {k} ‚Üí {v}")

            # --- √âtape 1 : fusion des Tags et TagMap (utilise location_id_map) ---
            try:
                tag_id_map, tagmap_id_map = merge_tags_and_tagmap(
                    merged_db_path,
                    file1_db,
                    file2_db,
                    note_mapping,
                    location_id_map,
                    item_id_map
                )
                print(f"Tag ID Map: {tag_id_map}")
                print(f"TagMap ID Map: {tagmap_id_map}")

            except Exception as e:
                import traceback
                print("‚ùå √âchec de merge_tags_and_tagmap (mais on continue le merge global) :")
                print(f"Exception captur√©e : {e}")
                traceback.print_exc()
                tag_id_map, tagmap_id_map = {}, {}

            print(f"Tag ID Map: {tag_id_map}")
            print(f"TagMap ID Map: {tagmap_id_map}")

            # 1Ô∏è‚É£ Mise √† jour des LocationId r√©siduels
            print("\n=== MISE √Ä JOUR DES LocationId R√âSIDUELS ===")
            merge_inputfields(merged_db_path, file1_db, file2_db, location_id_map)
            print("‚úî Fusion InputFields termin√©e")
            location_replacements_flat = {
                old_id: new_id
                for (_, old_id), new_id in sorted(location_id_map.items())
            }

            print("‚è≥ Appel de update_location_references...")
            try:
                update_location_references(merged_db_path, location_replacements_flat)
                print("‚úî Mise √† jour des r√©f√©rences LocationId termin√©e")
            except Exception as e:
                import traceback
                print(f"‚ùå ERREUR dans update_location_references : {e}")
                traceback.print_exc()

            with sqlite3.connect(merged_db_path) as conn:
                cleanup_playlist_item_location_map(conn)

            print("üü° Apr√®s update_location_references")
            sys.stdout.flush()
            time.sleep(0.5)
            print("üü¢ Avant suppression des tables MergeMapping_*")

            # 2Ô∏è‚É£ Suppression des tables MergeMapping_*
            print("\n=== SUPPRESSION DES TABLES MergeMapping_* ===")
            with sqlite3.connect(merged_db_path) as cleanup_conn:
                cleanup_conn.execute("PRAGMA busy_timeout = 5000")
                cur = cleanup_conn.cursor()
                cur.execute("""
                    SELECT name
                    FROM sqlite_master
                    WHERE type='table'
                      AND LOWER(name) LIKE 'mergemapping_%'
                """)
                rows = cur.fetchall()
                tables_to_drop = [row[0] for row in rows]
                print(f"üß™ R√©sultat brut de la requ√™te sqlite_master : {rows}")
                print(f"üßπ Tables MergeMapping_ d√©tect√©es : {tables_to_drop}")
                for tbl in tables_to_drop:
                    cur.execute(f"DROP TABLE IF EXISTS {tbl}")
                    print(f"‚úî Table supprim√©e : {tbl}")
                cleanup_conn.commit()

            # üîç V√©rification juste avant la copie
            print("üìÑ V√©rification taille et date de merged_userData.db juste avant la copie")
            print("üìç Fichier:", merged_db_path)
            print("üïí Modifi√© le:", os.path.getmtime(merged_db_path))
            print("üì¶ Taille:", os.path.getsize(merged_db_path), "octets")
            with sqlite3.connect(merged_db_path) as check_conn:
                cur = check_conn.cursor()
                cur.execute("SELECT name FROM sqlite_master WHERE name LIKE 'MergeMapping_%'")
                leftover = [row[0] for row in cur.fetchall()]
                print(f"üß™ Tables restantes juste avant la copie (v√©rification finale): {leftover}")

            print("üßπ Lib√©ration m√©moire et attente...")
            gc.collect()
            time.sleep(1.0)

            with sqlite3.connect(merged_db_path) as conn:
                conn.execute("DROP TABLE IF EXISTS PlaylistItemMediaMap")
                print("üóëÔ∏è Table PlaylistItemMediaMap supprim√©e avant VACUUM.")

            # 6Ô∏è‚É£ Cr√©ation d‚Äôune DB propre avec VACUUM INTO
            clean_filename = f"cleaned_{uuid.uuid4().hex}.db"
            clean_path = os.path.join(UPLOAD_FOLDER, clean_filename)

            print("üßπ VACUUM INTO pour g√©n√©rer une base nettoy√©e...")
            with sqlite3.connect(merged_db_path) as conn:
                conn.execute(f"VACUUM INTO '{clean_path}'")
            print(f"‚úÖ Fichier nettoy√© g√©n√©r√© : {clean_path}")

            # üß™ Cr√©ation d'une copie debug (juste pour toi)
            debug_copy_path = os.path.join(UPLOAD_FOLDER, "debug_cleaned_before_copy.db")
            shutil.copy(clean_path, debug_copy_path)
            print(f"üì§ Copie debug cr√©√©e : {debug_copy_path}")



            # 7Ô∏è‚É£ Copie vers destination finale officielle pour le frontend
            # ‚õî final_db_dest = os.path.join(UPLOAD_FOLDER, "userData.db")
            # ‚õî shutil.copy(clean_path, final_db_dest)
            # ‚õî print(f"‚úÖ Copie finale pour frontend : {final_db_dest}")

            # ‚úÖ On force l‚Äôusage uniquement du fichier debug (3 lignes d'ajout pour n'envoyer que le fichier)
            final_db_dest = os.path.join(UPLOAD_FOLDER, "debug_cleaned_before_copy.db")
            print("üö´ Copie vers userData.db d√©sactiv√©e ‚Äî envoi direct de debug_cleaned_before_copy.db")



            # ‚úÖ Forcer la g√©n√©ration des fichiers WAL et SHM sur userData.db
            try:
                print("üß™ Activation du mode WAL pour g√©n√©rer les fichiers -wal et -shm sur userData.db...")
                with sqlite3.connect(final_db_dest) as conn:
                    conn.execute("PRAGMA journal_mode=WAL;")
                    conn.execute("CREATE TABLE IF NOT EXISTS _Dummy (x INTEGER);")
                    conn.execute("INSERT INTO _Dummy (x) VALUES (1);")
                    conn.execute("DELETE FROM _Dummy;")
                    conn.execute("DROP TABLE IF EXISTS _Dummy;")  # Suppression finale
                    conn.commit()
                print("‚úÖ WAL/SHM g√©n√©r√©s et _Dummy supprim√©e sur userData.db")
            except Exception as e:
                print(f"‚ùå Erreur WAL/SHM sur userData.db: {e}")

            # 8Ô∏è‚É£ V√©rification finale dans userData.db
            with sqlite3.connect(final_db_dest) as final_check:
                cur = final_check.cursor()
                cur.execute("SELECT name FROM sqlite_master WHERE name LIKE 'MergeMapping_%'")
                tables_final = [row[0] for row in cur.fetchall()]
                # print("üìã Tables MergeMapping_ dans userData.db copi√© :", tables_final)
                print("üìã Tables MergeMapping_ dans debug_cleaned_before_copy.db :", tables_final)

            # √Ä la toute fin, juste avant return
            os.remove(os.path.join(UPLOAD_FOLDER, "merge_in_progress"))

            elapsed = time.time() - start_time
            print(f"‚è±Ô∏è Temps total du merge : {elapsed:.2f} secondes")

            # 5Ô∏è‚É£ Retour JSON final
            final_result = {
                "merged_file": "debug_cleaned_before_copy.db",
                "playlists": max_playlist_id,
                "merge_status": "done",
                "playlist_items": playlist_item_total,
                "media_files": max_media_id,
                "cleaned_items": orphaned_deleted,
                "integrity_check": integrity_result
            }
            sys.stdout.flush()
            print("üéØ R√©sum√© final pr√™t √† √™tre envoy√© au frontend.")
            print("üß™ Test acc√®s √† final_result:", final_result)
            return jsonify(final_result), 200

        except Exception as e:
            import traceback
            print("‚ùå Exception lev√©e pendant merge_data !")
            traceback.print_exc()
            return jsonify({"error": f"Erreur dans merge_data: {str(e)}"}), 500

    finally:
        if conn:
            try:
                conn.close()
            except:
                pass


# === üîí Ancienne m√©thode de g√©n√©ration ZIP backend (d√©sactiv√©e avec JSZip) ===

# def create_userdata_zip():
#     print("üßπ Cr√©ation du zip userData_only.zip apr√®s merge termin√©...")
#
#     zip_filename = "userData_only.zip"
#     zip_path = os.path.join(UPLOAD_FOLDER, zip_filename)
#
#     debug_db_path = os.path.join(UPLOAD_FOLDER, "debug_cleaned_before_copy.db")
#     shm_path = debug_db_path + "-shm"
#     wal_path = debug_db_path + "-wal"
#
#     with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_STORED) as zipf:
#         zipf.write(debug_db_path, arcname="userData.db")
#         if os.path.exists(shm_path):
#             zipf.write(shm_path, arcname="userData.db-shm")
#         if os.path.exists(wal_path):
#             zipf.write(wal_path, arcname="userData.db-wal")
#
#     print(f"‚úÖ Fichier ZIP final pr√™t : {zip_path}")
#
#
# @app.route("/create_zip_after_merge")
# def create_zip_after_merge():
#     start_time = time.time()
#
#     try:
#         create_userdata_zip()
#
#         # üîê Suppression du verrou juste apr√®s cr√©ation du ZIP
#         try:
#             os.remove(os.path.join(UPLOAD_FOLDER, "merge_in_progress"))
#             print("üßπ Verrou merge_in_progress supprim√© apr√®s cr√©ation ZIP.")
#         except FileNotFoundError:
#             print("‚ö†Ô∏è Aucun verrou √† supprimer : merge_in_progress absent.")
#
#         elapsed = time.time() - start_time
#         print(f"üì¶ Temps de cr√©ation du ZIP : {elapsed:.2f} secondes")
#
#         return jsonify({"status": "ZIP cr√©√© avec succ√®s"}), 200
#
#     except Exception as e:
#         print(f"‚ùå Erreur cr√©ation ZIP : {e}")
#         return jsonify({"error": str(e)}), 500
#
#
# @app.route("/download_userdata_zip")
# def download_userdata_zip():
#     zip_path = os.path.join(UPLOAD_FOLDER, "userData_only.zip")
#     if not os.path.exists(zip_path):
#         return jsonify({"error": "Fichier ZIP introuvable"}), 404
#     print(f"üì• Envoi du ZIP : {zip_path}")
#     return send_file(zip_path, as_attachment=True, download_name="userData_only.zip")


@app.route("/download_debug_db")
def download_debug_db():
    debug_path = os.path.join(UPLOAD_FOLDER, "debug_cleaned_before_copy.db")
    if not os.path.exists(debug_path):
        return jsonify({"error": "Fichier debug introuvable"}), 404
    print(f"üì• Envoi du fichier DEBUG : {debug_path}")
    return send_file(debug_path, as_attachment=True, download_name="userData.db")


@app.route("/download/debug")
def download_debug_copy():
    path = os.path.join(UPLOAD_FOLDER, "debug_cleaned_before_copy.db")
    if not os.path.exists(path):
        return jsonify({"error": "Fichier debug non trouv√©"}), 404
    return send_file(path, as_attachment=True, download_name="debug_cleaned_before_copy.db")


@app.route("/download/<filename>")
def download_file(filename):
    if os.path.exists(os.path.join(UPLOAD_FOLDER, "merge_in_progress")):
        print("üõë Tentative de t√©l√©chargement bloqu√©e : merge encore en cours.")
        return jsonify({"error": "Le fichier est encore en cours de cr√©ation"}), 503

    # allowed_files = {"userData.db", "userData.db-shm", "userData.db-wal"}
    allowed_files = {
        "debug_cleaned_before_copy.db",
        "debug_cleaned_before_copy.db-shm",
        "debug_cleaned_before_copy.db-wal"
    }

    if filename not in allowed_files:
        return jsonify({"error": "Fichier non autoris√©"}), 400

    path = os.path.join(UPLOAD_FOLDER, filename)
    if not os.path.exists(path):
        return jsonify({"error": "Fichier introuvable"}), 404

    print(f"üì• Envoi du fichier : {filename}")
    response = send_file(path, as_attachment=True)
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response


@app.errorhandler(Exception)
def handle_exception(e):
    response = jsonify({"error": str(e)})
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response, 500


import json
from flask import Response


@app.route("/")
def index():
    message = {"message": "Le serveur Flask fonctionne üéâ"}
    return Response(
        response=json.dumps(message, ensure_ascii=False),
        status=200,
        mimetype='application/json'
    )


# Fichier local pour stocker les stats
MERGE_STATS_FILE = "merge_stats.json"
STATS_LOCK = threading.Lock()


import json
import os


def load_merge_stats():
    if not os.path.exists(MERGE_STATS_FILE):
        return {"success": 0, "error": 0}
    with open(MERGE_STATS_FILE, "r") as f:
        return json.load(f)


def save_merge_stats(stats):
    with open(MERGE_STATS_FILE, "w") as f:
        json.dump(stats, f)


@app.route("/track-merge", methods=["POST"])
def track_merge():
    try:
        data = request.get_json()
        status = data.get("status")
        if status not in ("success", "error"):
            return jsonify({"error": "Invalid status"}), 400

        with STATS_LOCK:
            stats = load_merge_stats()
            if status == "error":
                error_message = data.get("message", "Erreur inconnue")
                if "errors" not in stats:
                    stats["errors"] = []
                stats["errors"].append(error_message)
                stats["error"] = stats.get("error", 0) + 1
            else:
                stats["success"] = stats.get("success", 0) + 1

            save_merge_stats(stats)

        return jsonify({"message": f"{status} count updated"}), 200
    except Exception as e:
        print("‚ùå Erreur dans /track-merge :", e)
        return jsonify({"error": str(e)}), 500


@app.route("/get-merge-stats", methods=["GET"])
def get_merge_stats():
    with STATS_LOCK:
        stats = load_merge_stats()

    return Response(
        response=json.dumps(stats, ensure_ascii=False, indent=2),
        status=200,
        mimetype='application/json'
    )


if __name__ == '__main__':
    port = int(os.environ.get("PORT", 8000))
    app.run(host='0.0.0.0', port=port)

